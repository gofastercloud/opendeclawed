# ============================================================================
# OpenDeclawed - Security-First Docker Deployment
# ============================================================================
# A production-grade, security-hardened deployment for OpenClaw, an AI agent
# platform. All values are parameterized via environment variables with
# sensible defaults. See .env.example for configuration options.
#
# Security Architecture:
#
#  +---------------------------------------------------------------------+
#  |                      CLOUDFLARE TUNNEL (Optional)                    |
#  |                     Profile: "tunnel" (opt-in)                      |
#  |                        cloudflared <-> Tunnel                       |
#  +-----------------------------+---------------------------------------+
#                                |
#  +-----------------------------v---------------------------------------+
#  |                       opendeclawed-gateway                          |
#  |  (REST API, WebSocket, health checks)                               |
#  |  Networks: opendeclawed-internal + opendeclawed-egress              |
#  |  Hardening: drop all caps, no_new_privileges, user 1000:1000        |
#  |  Healthcheck: curl -sf http://127.0.0.1:18789/health               |
#  +-----------------------------+---------------------------------------+
#                                |
#  +-----------------------------v---------------------------------------+
#  |                       opendeclawed-litellm                          |
#  |  OpenAI-compatible proxy -- single /v1 endpoint for all backends    |
#  |  Routes chat + embed -> Ollama / external backends                  |
#  |  Internal-only network, all LLM call logs visible in Dozzle         |
#  +-----------------------------+---------------------------------------+
#                                |
#                  +-------------v-----------------+
#                  |  Ollama / external backends    |
#                  |  (host network or remote API)  |
#                  +-------------------------------+
#
#  +---------------------------------------------------------------------+
#  |                       EGRESS FIREWALL (Init)                         |
#  |  DOCKER-USER iptables chain: DROP RFC1918, enforces egress control  |
#  |  Allows: established/related, DNS, inter-container, Docker bridge   |
#  |  Blocks: 10.0.0.0/8, 192.168.0.0/16, 169.254.0.0/16, multicast    |
#  +---------------------------------------------------------------------+
#
# NETWORK ISOLATION:
#  - opendeclawed-internal: bridge, internal=true (no internet access)
#  - opendeclawed-egress: bridge, allows outbound via egress firewall rules
#  - Services drop ALL Linux capabilities, run as unprivileged user 65534
#  - Read-only filesystems with minimal tmpfs mounts where needed
#
# PROFILES:
#  - Default: Local-only mode (gateway on 127.0.0.1 via port mapping)
#  - "tunnel": Enable Cloudflare tunnel ingress (cloudflared service)
#  - "tailscale": Enable Tailscale mesh VPN ingress (WireGuard-based)
#  - "monitor": Enable Watchtower + Dozzle (auto-update + log viewer)
#  - "cli": Enable openclaw-cli for onboarding/debugging (interactive)
#
# USAGE:
#  # Local mode (monitoring always included):
#  docker-compose up -d
#
#  # With Cloudflare tunnel (requires CLOUDFLARE_TOKEN):
#  docker-compose --profile tunnel up -d
#
#  # With Tailscale VPN:
#  docker-compose --profile tailscale up -d
#
#  # With CLI access (for onboarding):
#  docker-compose --profile cli up -d openclaw-cli
#
# ============================================================================

# Fix #14: DRY anchor for hardened container defaults.
# Applied to all services except egress-firewall (which needs NET_ADMIN/NET_RAW
# and root). Services override individual keys as needed after the merge.
x-hardened-defaults: &hardened-defaults
  security_opt:
    - no-new-privileges:true
  cap_drop:
    - ALL
  ipc: private
  restart: unless-stopped
  logging:
    driver: json-file
    options:
      max-size: 10m
      max-file: "3"

services:
  # -------------------------------------------------------------------
  # EGRESS FIREWALL: Kernel-level egress control via iptables
  # -------------------------------------------------------------------
  # Runs once at initialization to install DOCKER-USER rules that prevent
  # containers from reaching RFC1918 private networks and other restricted
  # ranges. This is the foundational security layer for all outbound traffic.
  #
  # Rules installed:
  #  - DROP to 10.0.0.0/8 (private)
  #  - DROP to 192.168.0.0/16 (private)
  #  - DROP to 169.254.0.0/16 (link-local)
  #  - DROP to 224.0.0.0/4 (multicast)
  #  - DROP to 240.0.0.0/4 (reserved)
  #  - DROP to gateway IP (prevents container->host bypass)
  #  - ACCEPT established/related connections (stateful)
  #  - ACCEPT DNS on :53 to blocky IP only (required for container operation)
  #  - ACCEPT to openclaw-egress network (inter-container)
  #  - ACCEPT to docker0 bridge (Docker infrastructure)
  # SECURITY NOTE: This container runs as a PERSISTENT SIDECAR (not init).
  # It re-checks and re-installs rules every 5s, surviving Docker daemon
  # restarts. If Docker restarts, containers come back via restart: unless-stopped
  # but DOCKER-USER rules are flushed. The loop catches this within 5s.
  egress-firewall:
    image: "${ALPINE_IMAGE:-alpine:3.19}"
    container_name: opendeclawed-egress-firewall
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    cap_drop:
      - ALL
    # NOTE: no-new-privileges, read_only, and non-root user are intentionally
    # omitted — iptables requires root and apk install writes to the filesystem.
    # Mitigated by: Alpine image pinned by tag, Trivy scanning, minimal surface.
    security_opt:
      - no-new-privileges:true
    ipc: private
    restart: unless-stopped
    entrypoint: |
      /bin/sh -c '
        apk add --no-cache iptables ip6tables || { echo "FATAL: Cannot install iptables"; exit 1; }

        GATEWAY_IP="$${GATEWAY_IP:-172.17.0.1}"
        EGRESS_SUBNET="$${EGRESS_SUBNET:-172.28.0.0/24}"
        INTERNAL_SUBNET="$${INTERNAL_SUBNET:-172.27.0.0/24}"
        BLOCKY_IP="$${BLOCKY_IP:-172.27.0.53}"
        BLOCKY_EGRESS_IP="172.28.0.53"

        install_rules() {
            # Sentinel check: verify key rules to confirm full ruleset is present.
            if iptables -C DOCKER-USER -d 10.0.0.0/8 -j DROP 2>/dev/null && \
               iptables -C DOCKER-USER -p udp --dport 53 -j DROP 2>/dev/null && \
               iptables -C DOCKER-USER -s $$BLOCKY_EGRESS_IP -d 8.8.8.8 -j ACCEPT 2>/dev/null; then
                return 0
            fi

            echo "[$(date -u +%H:%M:%S)] Installing iptables rules..."

            # --- IPv4: Flush and rebuild DOCKER-USER ---
            # Note: atomic iptables -E (rename) fails in Docker/OrbStack because
            # Docker holds a jump reference to DOCKER-USER. Flush-and-rebuild has
            # a brief window with no rules but is reliable. The 5s re-check loop
            # ensures rules are reinstalled if Docker resets the chain.
            iptables -N DOCKER-USER 2>/dev/null || true
            iptables -F DOCKER-USER

            # Clean up any leftover temp chains from previous atomic swap attempts
            iptables -F DOCKER-USER-NEW 2>/dev/null || true
            iptables -X DOCKER-USER-NEW 2>/dev/null || true
            iptables -F DOCKER-USER-OLD 2>/dev/null || true
            iptables -X DOCKER-USER-OLD 2>/dev/null || true

            # STATEFUL: Accept established/related
            iptables -A DOCKER-USER -m state --state ESTABLISHED,RELATED -j ACCEPT

            # ── DNS: Only allow Blocky and Docker internal resolver ──
            iptables -A DOCKER-USER -p udp --dport 53 -d $BLOCKY_IP -j ACCEPT
            iptables -A DOCKER-USER -p tcp --dport 53 -d $BLOCKY_IP -j ACCEPT
            iptables -A DOCKER-USER -p udp --dport 53 -d 127.0.0.11 -j ACCEPT
            iptables -A DOCKER-USER -p tcp --dport 53 -d 127.0.0.11 -j ACCEPT
            # DROP all other outbound DNS (prevents bypassing Blocky)
            iptables -A DOCKER-USER -p udp --dport 53 -j DROP
            iptables -A DOCKER-USER -p tcp --dport 53 -j DROP

            # ── Block well-known public DNS resolvers (DoH/DoT prevention) ──
            # These resolvers offer DNS-over-HTTPS on port 443, which would
            # bypass both Blocky and the port 53 DROP above.
            # Blocky itself needs DoH access to its upstream resolvers, so
            # exempt its egress IP from these blocks.
            for resolver in \
                8.8.8.8 8.8.4.4 \
                1.1.1.1 1.0.0.1 \
                9.9.9.9 149.112.112.112 \
                208.67.222.222 208.67.220.220 \
                94.140.14.14 94.140.15.15 \
                76.76.2.0 76.76.10.0 \
            ; do
                iptables -A DOCKER-USER -s $$BLOCKY_EGRESS_IP -d $$resolver -j ACCEPT
                iptables -A DOCKER-USER -d $$resolver -j DROP
            done

            # ── ALLOW specific inter-container traffic (openclaw-internal) ──
            iptables -A DOCKER-USER -s $INTERNAL_SUBNET -d $INTERNAL_SUBNET -p tcp --dport 4000 -j ACCEPT    # Gateway->LiteLLM
            iptables -A DOCKER-USER -s $INTERNAL_SUBNET -d $INTERNAL_SUBNET -p tcp --dport 2375 -j ACCEPT    # Watchtower/Dozzle->Socket-proxy
            iptables -A DOCKER-USER -s $INTERNAL_SUBNET -d $INTERNAL_SUBNET -p tcp --dport $${GATEWAY_PORT:-18789} -j ACCEPT  # Cloudflared/Tailscale->Gateway
            iptables -A DOCKER-USER -s $INTERNAL_SUBNET -d $INTERNAL_SUBNET -p tcp --dport 5005 -j ACCEPT    # Cloudflared->Dozzle (tunnel /dozzle route)
            iptables -A DOCKER-USER -s $INTERNAL_SUBNET -d $INTERNAL_SUBNET -p tcp --dport $${SEARXNG_PORT:-8080} -j ACCEPT  # Gateway->SearXNG
            iptables -A DOCKER-USER -s $INTERNAL_SUBNET -d $INTERNAL_SUBNET -p icmp -j ACCEPT                # Health checks

            # ── ALLOW specific inter-container traffic (openclaw-egress) ──
            iptables -A DOCKER-USER -s $EGRESS_SUBNET -d $EGRESS_SUBNET -p tcp --dport $${GATEWAY_PORT:-18789} -j ACCEPT
            iptables -A DOCKER-USER -s $EGRESS_SUBNET -d $EGRESS_SUBNET -p icmp -j ACCEPT

            # DROP gateway IP BEFORE Docker bridge ACCEPT
            iptables -A DOCKER-USER -d $GATEWAY_IP -j DROP

            # ALLOW Docker bridge (narrowed to /24)
            iptables -A DOCKER-USER -d 172.17.0.0/24 -j ACCEPT
            iptables -A DOCKER-USER -s 172.17.0.0/24 -j ACCEPT

            # DROP RFC1918
            iptables -A DOCKER-USER -d 10.0.0.0/8 -j DROP
            iptables -A DOCKER-USER -d 192.168.0.0/16 -j DROP
            iptables -A DOCKER-USER -d 172.16.0.0/12 -j DROP

            # DROP link-local, multicast, reserved
            iptables -A DOCKER-USER -d 169.254.0.0/16 -j DROP
            iptables -A DOCKER-USER -d 224.0.0.0/4 -j DROP
            iptables -A DOCKER-USER -d 240.0.0.0/4 -j DROP

            # Default pass-through (must be last)
            iptables -A DOCKER-USER -j RETURN

            # --- IPv6: Flush and rebuild ---
            ip6tables -N DOCKER-USER 2>/dev/null || true
            ip6tables -F DOCKER-USER

            # Clean up leftover temp chains
            ip6tables -F DOCKER-USER-NEW 2>/dev/null || true
            ip6tables -X DOCKER-USER-NEW 2>/dev/null || true
            ip6tables -F DOCKER-USER-OLD 2>/dev/null || true
            ip6tables -X DOCKER-USER-OLD 2>/dev/null || true

            ip6tables -A DOCKER-USER -m state --state ESTABLISHED,RELATED -j ACCEPT

            # DROP all IPv6 DNS (blocky listens on IPv4 only)
            ip6tables -A DOCKER-USER -p udp --dport 53 -j DROP
            ip6tables -A DOCKER-USER -p tcp --dport 53 -j DROP

            # DROP IPv6 private/link-local/multicast
            ip6tables -A DOCKER-USER -d fc00::/7 -j DROP
            ip6tables -A DOCKER-USER -d fe80::/10 -j DROP
            ip6tables -A DOCKER-USER -d ff00::/8 -j DROP

            # Default pass-through (must be last)
            ip6tables -A DOCKER-USER -j RETURN

            echo "[$(date -u +%H:%M:%S)] Egress firewall rules installed"
        }

        # Install immediately, then re-check every 60s in case Docker resets the chain
        install_rules
        while true; do
            sleep 60
            install_rules
        done
      '
    healthcheck:
      test: ["CMD-SHELL", "iptables -C DOCKER-USER -d 10.0.0.0/8 -j DROP 2>/dev/null && iptables -C DOCKER-USER -p udp --dport 53 -j DROP 2>/dev/null && ip6tables -C DOCKER-USER -p udp --dport 53 -j DROP 2>/dev/null"]
      interval: 10s
      timeout: 5s
      retries: 6
      start_period: 15s
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: "32m"
          pids: 32
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: "3"

  # -------------------------------------------------------------------
  # LITELLM: OpenAI-compatible LLM proxy/router
  # -------------------------------------------------------------------
  # Presents a single /v1/chat/completions and /v1/embeddings endpoint
  # to OpenClaw, abstracting the actual backend (Ollama, MLX, vLLM,
  # cloud APIs, etc.). Backend routing is defined in
  # litellm_config.yaml -- swap backends without changing openclaw.json.
  #
  # Profile: always-on (core infrastructure)
  #
  # Security:
  #  - Internal + egress networks (egress needed to reach Anthropic API)
  #  - DNS via blocky (threat filtering + DoH privacy)
  #  - read-only root, all caps dropped, no_new_privileges
  #  - ANTHROPIC_API_KEY isolated here — never passed to gateway
  #  - Master key must be set via environment (fails fast if missing)
  #  - No database (stateless proxy -- logs to stdout for Dozzle)
  litellm:
    <<: *hardened-defaults
    image: "${LITELLM_IMAGE:-ghcr.io/berriai/litellm:main-stable}"
    container_name: opendeclawed-litellm
    command: ["--config", "/app/config.yaml", "--port", "${LITELLM_PORT:-4000}"]
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - ${LITELLM_CONFIG:-./litellm_config.yaml}:/app/config.yaml:ro
    environment:
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:?ANTHROPIC_API_KEY must be set - run setup.sh}
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:?LITELLM_MASTER_KEY must be set - run setup.sh}
      - LITELLM_LOG_LEVEL=${LOG_LEVEL:-info}
    deploy:
      resources:
        limits:
          cpus: "${LITELLM_CPUS:-1}"
          memory: "${LITELLM_MEM:-512m}"
          pids: 256
        reservations:
          cpus: "0.25"
          memory: "128m"
    read_only: true
    tmpfs:
      - /tmp:size=256m,noexec,nosuid,nodev
    user: "65534:65534"
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:${LITELLM_PORT:-4000}/health/liveliness')"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 20s
    depends_on:
      egress-firewall:
        condition: service_healthy
      blocky:
        condition: service_healthy

  # -------------------------------------------------------------------
  # OPENCLAW-GATEWAY: API server orchestrating LLM backends
  # -------------------------------------------------------------------
  # Central OpenClaw gateway service. Hosts REST API, WebSocket, and health
  # checks. Bridges internal (LiteLLM proxy) and egress (outbound) networks.
  #
  # Networking:
  #  - openclaw-internal: Can reach litellm:4000 (LLM router)
  #  - openclaw-egress: Subject to egress firewall rules (no RFC1918)
  #  - Ports bound to 127.0.0.1 by default (local-only); tunnel mode disables
  #
  # Healthcheck:
  #  - curl -sf http://127.0.0.1:${GATEWAY_PORT:-18789}/health
  #  - Fails if gateway crashes or becomes unresponsive
  #  - Required for cloudflared dependency: service_healthy
  #
  # Security:
  #  - read-only filesystem with tmpfs for Node.js cache dirs
  #  - Persistent state via bind mounts to ~/.openclaw
  #  - Container sees ONLY config + workspace directories, no other host paths
  #  - no_new_privileges, drop all capabilities, unprivileged user
  #  - ipc: private (isolated)
  openclaw-gateway:
    <<: *hardened-defaults
    image: "${OPENCLAW_IMAGE:-openclaw:local}"
    container_name: opendeclawed-gateway
    # Upstream CMD: node openclaw.mjs gateway --allow-unconfigured
    # Built image uses dist/index.js. We add --bind lan for container networking.
    command: ["node", "dist/index.js", "gateway", "--allow-unconfigured", "--bind", "lan", "--port", "${GATEWAY_PORT:-18789}"]
    init: true
    # Route DNS through blocky for threat filtering + DoH privacy.
    # Docker dns: requires an IP, not a hostname. Blocky is pinned to
    # 172.27.0.53 on the internal network (see blocky service below).
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    ports:
      # Local mode: bind to 127.0.0.1 (default)
      # Tunnel mode: no port binding (cloudflared proxies)
      - "127.0.0.1:${GATEWAY_PORT:-18789}:${GATEWAY_PORT:-18789}"
    volumes:
      # Persistent config -- bind mount to host ~/.openclaw (survives daemon restarts).
      # Matches upstream: /home/node/.openclaw is the default config path.
      - ${OPENCLAW_CONFIG_DIR:-~/.openclaw}:/home/node/.openclaw:rw
      - ${OPENCLAW_WORKSPACE_DIR:-~/.openclaw/workspace}:/home/node/.openclaw/workspace:rw
    tmpfs:
      # 512m: covers Node.js temp files, no large file processing expected
      - /tmp:size=512m,noexec,nosuid,nodev
      - /run:size=512m,noexec,nosuid,nodev
      - /app/.cache:size=256m,noexec,nosuid,nodev
      - /home/node/.npm:size=128m,noexec,nosuid,nodev
    environment:
      - HOME=/home/node
      - OPENCLAW_GATEWAY_TOKEN=${OPENCLAW_GATEWAY_TOKEN:-}
      - LITELLM_HOST=litellm
      - LITELLM_PORT=${LITELLM_PORT:-4000}
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY:?LITELLM_MASTER_KEY must be set - run setup.sh}
      - GATEWAY_PORT=${GATEWAY_PORT:-18789}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    deploy:
      resources:
        limits:
          cpus: "${GATEWAY_CPUS:-2}"
          memory: "${GATEWAY_MEM:-4g}"
          pids: 256
        reservations:
          cpus: "${GATEWAY_CPUS_RESERVE:-1}"
          memory: "${GATEWAY_MEM_RESERVE:-2g}"
    read_only: true
    # Upstream image sets USER node (uid 1000). Must match for /app file access.
    user: "1000:1000"
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://127.0.0.1:${GATEWAY_PORT:-18789}/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    depends_on:
      egress-firewall:
        condition: service_healthy
      litellm:
        condition: service_healthy
      blocky:
        condition: service_healthy

  # -------------------------------------------------------------------
  # CLOUDFLARED: Cloudflare Tunnel ingress (optional, profile: tunnel)
  # -------------------------------------------------------------------
  # Establishes a secure tunnel to Cloudflare, making openclaw-gateway
  # accessible via Cloudflare's global network without exposing host ports.
  #
  # Profile: "tunnel"
  #  - Enable with: docker-compose --profile tunnel up -d
  #  - Requires CLOUDFLARE_TOKEN env var
  #
  # Security:
  #  - Depends on: egress-firewall (healthy), openclaw-gateway (healthy)
  #  - read-only filesystem
  #  - NO capabilities at all (cap_drop: [ALL], no cap_add)
  #  - user 65534:65534 (nobody:nogroup)
  #  - ipc: private (isolated)
  #  - Service will not start until gateway is healthy
  #
  # Configuration:
  #  - CLOUDFLARE_TOKEN: Required for tunnel auth (get from Cloudflare dashboard)
  #  - CLOUDFLARE_TUNNEL_NAME: Tunnel ID or name
  #  - CLOUDFLARE_TUNNEL_ROUTE: Hostname to route (e.g., openclaw.example.com)
  cloudflared:
    <<: *hardened-defaults
    image: "${CLOUDFLARED_IMAGE:-cloudflare/cloudflared:2025.2.0}"
    container_name: opendeclawed-cloudflared
    profiles:
      - tunnel
    # Token-based tunnels: configure ingress rules in Cloudflare Zero Trust
    # dashboard under Networks > Tunnels > Public Hostname.
    # The cloudflared image is distroless (no shell), so we pass the token
    # via TUNNEL_TOKEN env var and use the binary's CLI directly.
    command: ["tunnel", "run"]
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
    read_only: true
    user: "65534:65534"
    environment:
      - CLOUDFLARE_TOKEN=${CLOUDFLARE_TOKEN:-}
      # Fix #28: cloudflared reads TUNNEL_TOKEN from env instead of CLI arg
      - TUNNEL_TOKEN=${CLOUDFLARE_TOKEN:-}
    depends_on:
      egress-firewall:
        condition: service_healthy
      openclaw-gateway:
        condition: service_healthy
      blocky:
        condition: service_started

  # -------------------------------------------------------------------
  # TAILSCALE: Zero-config mesh VPN ingress (optional, profile: tailscale)
  # -------------------------------------------------------------------
  # Exposes openclaw-gateway via your Tailscale tailnet. Devices on the
  # same tailnet can reach the gateway at https://<hostname>.<tailnet>.ts.net.
  # Uses Tailscale Serve to proxy HTTPS -> gateway HTTP internally.
  #
  # Profile: "tailscale"
  #  - Enable with: docker compose --profile tailscale up -d
  #  - Requires TS_AUTHKEY env var (one-time or reusable auth key)
  #
  # Security:
  #  - Traffic encrypted end-to-end via WireGuard (Tailscale)
  #  - Access controlled by Tailscale ACLs (your admin console)
  #  - No public ports exposed; only tailnet members can connect
  #  - Persistent state volume avoids re-auth on restart
  #  - cap_add: NET_ADMIN + NET_RAW (required for WireGuard tunnel)
  #  - Read-only root, all other caps dropped
  #
  # Configuration:
  #  - TS_AUTHKEY: Auth key from https://login.tailscale.com/admin/settings/keys
  #  - TS_HOSTNAME: Machine name on your tailnet (default: "openclaw")
  #  - TS_EXTRA_ARGS: Additional tailscaled flags (optional)
  tailscale:
    <<: *hardened-defaults
    image: "${TAILSCALE_IMAGE:-tailscale/tailscale:v1.76.6}"
    container_name: opendeclawed-tailscale
    profiles:
      - tailscale
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
    volumes:
      - tailscale-state:/var/lib/tailscale
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY:-}
      - TS_HOSTNAME=${TS_HOSTNAME:-openclaw}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_SERVE_CONFIG=/config/serve.json
      - TS_USERSPACE=true
      - TS_EXTRA_ARGS=${TS_EXTRA_ARGS:-}
    # Tailscale Serve config: proxy HTTPS on :443 -> gateway HTTP
    # TS_CERT_DOMAIN defaults to ts.net; override for custom Tailscale domains
    # (e.g. if using a custom DERP server or vanity domain).
    entrypoint: |
      /bin/sh -c '
        mkdir -p /config
        cat > /config/serve.json << SERVECFG
        {
          "TCP": {
            "443": {
              "HTTPS": true
            }
          },
          "Web": {
            "$${TS_HOSTNAME:-openclaw}.$${TS_CERT_DOMAIN:-ts.net}:443": {
              "Handlers": {
                "/": {
                  "Proxy": "http://openclaw-gateway:${GATEWAY_PORT:-18789}"
                }
              }
            }
          }
        }
      SERVECFG
        exec /usr/local/bin/containerboot
      '
    deploy:
      resources:
        limits:
          cpus: "${TAILSCALE_CPUS:-0.5}"
          memory: "${TAILSCALE_MEM:-128m}"
          pids: 128
    read_only: true
    cap_add:
      - NET_ADMIN
      - NET_RAW
    # NOTE: user directive omitted -- Tailscale needs root for WireGuard tunnel setup.
    # Mitigated by: read_only, no-new-privileges, cap_drop ALL + selective cap_add.
    depends_on:
      egress-firewall:
        condition: service_healthy
      openclaw-gateway:
        condition: service_healthy
      blocky:
        condition: service_started
    labels:
      - "com.centurylinklabs.watchtower.enable=false"

  # -------------------------------------------------------------------
  # DOCKER-SOCKET-PROXY: Least-privilege Docker API access for Watchtower
  # -------------------------------------------------------------------
  # Instead of giving Watchtower direct access to /var/run/docker.sock
  # (which is equivalent to root on the host), we proxy only the specific
  # Docker API endpoints Watchtower needs: container list, image pull,
  # and container restart. Everything else is denied.
  #
  # Security:
  #  - Only exposes GET/POST to containers, images, version, events
  #  - Denies: exec, volumes, networks, secrets, swarm, build, commit
  #  - Internal network only (no internet access)
  #  - cap_drop ALL, no_new_privileges
  docker-socket-proxy:
    <<: *hardened-defaults
    image: "${SOCKET_PROXY_IMAGE:-tecnativa/docker-socket-proxy:0.2.0}"
    container_name: opendeclawed-socket-proxy
    # Monitoring is default (no profile gate) — socket-proxy, watchtower, dozzle
    # always start to ensure operational visibility.
    networks:
      openclaw-internal: {}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      # Allow only what Watchtower needs
      - CONTAINERS=1
      - IMAGES=1
      - VERSION=1
      - EVENTS=1
      - POST=1
      # Deny everything else
      - AUTH=0
      - BUILD=0
      - COMMIT=0
      - CONFIGS=0
      - DISTRIBUTION=0
      - EXEC=0
      - GRPC=0
      - INFO=1
      - NETWORKS=0
      - NODES=0
      - PLUGINS=0
      - SECRETS=0
      - SERVICES=0
      - SESSION=0
      - SWARM=0
      - SYSTEM=0
      - TASKS=0
      - VOLUMES=0
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "64m"
          pids: 64
    # NOTE: user directive omitted -- socket proxy needs root to bind port 2375.
    # NOTE: read_only omitted -- entrypoint generates haproxy.cfg from template
    # at /usr/local/etc/haproxy/ on startup. Mitigated by cap_drop ALL +
    # no-new-privileges + Docker socket being the actual security boundary.
    tmpfs:
      - /tmp:size=16m,noexec,nosuid,nodev
      - /run:size=16m,noexec,nosuid,nodev
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:2375/version || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    labels:
      - "com.centurylinklabs.watchtower.enable=false"

  # -------------------------------------------------------------------
  # BLOCKY: DNS firewall with threat intelligence blocklists
  # -------------------------------------------------------------------
  # Lightweight DNS proxy that filters queries against known-malicious
  # domains (malware C2, phishing, cryptomining, trackers). Upstream
  # resolvers use DNS-over-HTTPS for privacy. All containers on the
  # egress network use this as their DNS server.
  #
  # Profile: always-on (no profile -- runs with default stack)
  #
  # Security:
  #  - Blocks known-malicious domains at DNS layer
  #  - Upstream via DoH (DNS-over-HTTPS) -- no plaintext DNS to ISP
  #  - Internal network only for serving; egress for upstream DoH
  #  - cap_drop ALL, no_new_privileges, read-only root
  #  - Provides defense against DNS exfiltration tunneling via rate limits
  blocky:
    <<: *hardened-defaults
    image: "${BLOCKY_IMAGE:-spx01/blocky:v0.24}"
    container_name: opendeclawed-blocky
    command: ["--config", "/app/config/config.yml"]
    networks:
      openclaw-internal:
        ipv4_address: "${BLOCKY_IP:-172.27.0.53}"
      openclaw-egress:
        ipv4_address: 172.28.0.53
    volumes:
      - ./examples/blocky-config.yml:/app/config/config.yml:ro
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
    deploy:
      resources:
        limits:
          cpus: "${BLOCKY_CPUS:-0.5}"
          memory: "${BLOCKY_MEM:-64m}"
          pids: 64
    read_only: true
    cap_add:
      - NET_BIND_SERVICE
    user: "65534:65534"
    healthcheck:
      # Use blocky's built-in healthcheck (DNS query on port 53)
      test: ["CMD", "/app/blocky", "healthcheck", "--port", "53"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    depends_on:
      egress-firewall:
        condition: service_healthy
    labels:
      - "com.centurylinklabs.watchtower.enable=true"

  # -------------------------------------------------------------------
  # SEARXNG: Self-hosted metasearch engine for agent web search
  # -------------------------------------------------------------------
  # Privacy-respecting metasearch engine that aggregates results from
  # multiple search engines. Used as a workspace skill replacement for
  # OpenClaw's built-in web_search tool (which only supports Brave/Perplexity).
  #
  # Profile: always-on (core infrastructure for agent search)
  #
  # Security:
  #  - Internal network: gateway reaches searxng:8080
  #  - Egress network: SearXNG reaches internet for search queries
  #  - DNS via blocky (threat filtering + DoH privacy)
  #  - No host port binding (internal only)
  #  - read-only root, all caps dropped, no_new_privileges
  #  - Unprivileged user 65534:65534
  searxng:
    <<: *hardened-defaults
    image: "${SEARXNG_IMAGE:-searxng/searxng:latest}"
    container_name: opendeclawed-searxng
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
    volumes:
      - ./examples/searxng-settings.yml:/etc/searxng/settings.yml:ro
    environment:
      - SEARXNG_SECRET=${SEARXNG_SECRET_KEY:-}
    tmpfs:
      - /etc/searxng:size=16m,noexec,nosuid,nodev
      - /tmp:size=64m,noexec,nosuid,nodev
    deploy:
      resources:
        limits:
          cpus: "${SEARXNG_CPUS:-0.5}"
          memory: "${SEARXNG_MEM:-256m}"
          pids: 128
    read_only: true
    user: "65534:65534"
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://127.0.0.1:${SEARXNG_PORT:-8080}/healthz || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 15s
    depends_on:
      egress-firewall:
        condition: service_healthy
      blocky:
        condition: service_healthy
    labels:
      - "com.centurylinklabs.watchtower.enable=true"

  # -------------------------------------------------------------------
  # WATCHTOWER: Automatic container image updates (optional, profile: monitor)
  # -------------------------------------------------------------------
  # Monitors running containers and updates them when new images are
  # available. Label-controlled: only updates containers with
  # com.centurylinklabs.watchtower.enable=true.
  #
  # Profile: "monitor"
  #  - Enable with: docker-compose --profile monitor up -d
  #
  # Security:
  #  - Requires /var/run/docker.sock (read-only where possible)
  #  - cap_drop ALL, no_new_privileges
  #  - Label-gated: won't touch unlabeled containers
  #  - Poll interval configurable (default 24h)
  #  - Read-only root filesystem
  watchtower:
    <<: *hardened-defaults
    image: "${WATCHTOWER_IMAGE:-containrrr/watchtower:1.7.1}"
    container_name: opendeclawed-watchtower
    networks:
      openclaw-internal: {}
    # SECURITY: No direct Docker socket access. Uses docker-socket-proxy
    # which only exposes container/image API endpoints.
    environment:
      - DOCKER_HOST=tcp://docker-socket-proxy:2375
      - WATCHTOWER_LABEL_ENABLE=true
      - WATCHTOWER_POLL_INTERVAL=${WATCHTOWER_POLL_SECONDS:-86400}
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_INCLUDE_STOPPED=false
      - WATCHTOWER_NO_RESTART=false
      - WATCHTOWER_ROLLING_RESTART=true
      - WATCHTOWER_NOTIFICATIONS=${WATCHTOWER_NOTIFICATIONS:-}
      - WATCHTOWER_NOTIFICATION_URL=${WATCHTOWER_NOTIFICATION_URL:-}
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
    deploy:
      resources:
        limits:
          cpus: "${WATCHTOWER_CPUS:-0.5}"
          memory: "${WATCHTOWER_MEM:-128m}"
          pids: 64
    read_only: true
    # NOTE: user directive omitted -- Watchtower image runs as root by default
    # and needs it for Docker API interactions. Mitigated by socket proxy.
    depends_on:
      docker-socket-proxy:
        condition: service_healthy
    labels:
      - "com.centurylinklabs.watchtower.enable=false"

  # -------------------------------------------------------------------
  # DOZZLE: Real-time container log viewer (monitor profile)
  # -------------------------------------------------------------------
  # Lightweight web UI for viewing, filtering, and searching container
  # logs in real time. Reads Docker's native json-file logs -- no log
  # driver changes or sidecars required. Read-only Docker socket access.
  #
  # Profile: "monitor" (same as Watchtower)
  #
  # Security:
  #  - Docker API via socket-proxy (no direct socket mount -- prevents secret leakage via docker inspect)
  #  - DOZZLE_FILTER: only shows openclaw-* containers
  #  - DOZZLE_NO_ANALYTICS: disables telemetry to Dozzle maintainers
  #  - Port bound to 127.0.0.1 (local access only)
  #  - read-only root, all caps dropped, no_new_privileges
  dozzle:
    <<: *hardened-defaults
    image: "${DOZZLE_IMAGE:-amir20/dozzle:v8.8.2}"
    container_name: opendeclawed-dozzle
    networks:
      openclaw-internal: {}
    # SECURITY: No direct Docker socket. Uses socket-proxy (same as Watchtower).
    # Dozzle only needs container list + events for log streaming.
    environment:
      - DOZZLE_REMOTE_HOST=tcp://docker-socket-proxy:2375
      - DOZZLE_ADDR=:${DOZZLE_PORT:-5005}
      - DOZZLE_LEVEL=info
      - DOZZLE_FILTER=name=opendeclawed-*
      - DOZZLE_NO_ANALYTICS=true
      - DOZZLE_BASE=/dozzle
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
      - 127.0.0.11  # Docker embedded DNS fallback for service discovery
    tmpfs:
      - /tmp:size=32m,noexec,nosuid,nodev
    ports:
      - "127.0.0.1:${DOZZLE_PORT:-5005}:${DOZZLE_PORT:-5005}"
    deploy:
      resources:
        limits:
          cpus: "${DOZZLE_CPUS:-0.25}"
          memory: "${DOZZLE_MEM:-64m}"
          pids: 64
    read_only: true
    user: "65534:65534"
    # Override logging: Dozzle generates minimal logs, smaller limits sufficient
    logging:
      driver: json-file
      options:
        max-size: 5m
        max-file: "2"
    depends_on:
      docker-socket-proxy:
        condition: service_healthy

  # -------------------------------------------------------------------
  # OPENCLAW-CLI: Interactive CLI for onboarding and debugging
  # -------------------------------------------------------------------
  # Interactive shell for running openclaw commands, onboarding workflows,
  # and debugging. Shares networks with gateway for API access.
  #
  # Profile: "cli"
  #  - Enable with: docker-compose --profile cli run openclaw-cli bash
  #  - Interactive (stdin_open: true, tty: true)
  #  - Grants NET_RAW for ping/traceroute if needed
  #
  # Security:
  #  - Runs as node user (1000:1000), matching upstream image
  #  - cap_add: [NET_RAW] for network diagnostics
  #  - Other caps still dropped, no_new_privileges, ipc isolated
  #  - Read-write home volume for credentials/config
  openclaw-cli:
    <<: *hardened-defaults
    image: "${OPENCLAW_IMAGE:-openclaw:local}"
    container_name: opendeclawed-cli
    entrypoint: ["node", "dist/index.js"]
    init: true
    profiles:
      - cli
    stdin_open: true
    tty: true
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    volumes:
      - ${OPENCLAW_CONFIG_DIR:-~/.openclaw}:/home/node/.openclaw:rw
      - ${OPENCLAW_WORKSPACE_DIR:-~/.openclaw/workspace}:/home/node/.openclaw/workspace:rw
    tmpfs:
      - /tmp:size=1g,noexec,nosuid,nodev
      - /run:size=256m,noexec,nosuid,nodev
    environment:
      - HOME=/home/node
      - TERM=xterm-256color
      - BROWSER=echo
      - OPENCLAW_GATEWAY_TOKEN=${OPENCLAW_GATEWAY_TOKEN:-}
      - LITELLM_HOST=litellm
      - LITELLM_PORT=${LITELLM_PORT:-4000}
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY:-sk-opendeclawed-internal}
      - GATEWAY_HOST=openclaw-gateway
      - GATEWAY_PORT=${GATEWAY_PORT:-18789}
      - LOG_LEVEL=${LOG_LEVEL:-debug}
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "2g"
          pids: 256
    cap_add:
      - NET_RAW
    user: "1000:1000"
    depends_on:
      egress-firewall:
        condition: service_healthy
      openclaw-gateway:
        condition: service_started

networks:
  # -------------------------------------------------------------------
  # OPENCLAW-INTERNAL: LLM service mesh (no internet access)
  # -------------------------------------------------------------------
  # Bridge network for litellm and openclaw-gateway
  # to communicate. Marked internal: true so containers cannot reach the host
  # or internet directly. Subject to egress firewall rules (allows
  # inter-container within 172.27.0.0/16).
  openclaw-internal:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: "${INTERNAL_SUBNET:-172.27.0.0/24}"

  # -------------------------------------------------------------------
  # OPENCLAW-EGRESS: Egress-controlled network (outbound rules apply)
  # -------------------------------------------------------------------
  # Bridge network for openclaw-gateway and cloudflared. Subject to strict
  # egress firewall rules: allows DNS, established/related, and inter-
  # container traffic; blocks RFC1918 private ranges and reserved IPs.
  # Used for cloudflared (tunnel) and controlled external API calls.
  openclaw-egress:
    driver: bridge
    ipam:
      config:
        - subnet: "${EGRESS_SUBNET:-172.28.0.0/24}"

volumes:
  # -------------------------------------------------------------------
  # TAILSCALE-STATE: Persistent Tailscale auth state
  # -------------------------------------------------------------------
  # Stores WireGuard keys and auth state. Persisting this avoids
  # re-authentication on container restart.
  tailscale-state:
    driver: local


  # -------------------------------------------------------------------
  # OPENCLAW HOME: Bind-mounted from host (no volume definition needed)
  # -------------------------------------------------------------------
  # The gateway and CLI containers bind-mount two host directories:
  #   ${OPENCLAW_CONFIG_DIR:-~/.openclaw}     -> /home/openclaw/config
  #   ${OPENCLAW_WORKSPACE_DIR:-~/.openclaw/workspace} -> /home/openclaw/workspace
  #
  # These are the ONLY host paths accessible to the gateway container.
  # Docker enforces this -- containers cannot mount paths not declared
  # in their compose volumes section (and we block Docker socket access
  # via the socket proxy, preventing runtime volume creation).
  #
  # Host-side permissions (set by setup.sh):
  #   ~/.openclaw/            -> 700 (owner only)
  #   ~/.openclaw/openclaw.json -> 600
  #   ~/.openclaw/workspace/  -> 700
