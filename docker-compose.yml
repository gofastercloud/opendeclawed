# ============================================================================
# OpenDeclawed - Security-First Docker Deployment
# ============================================================================
# A production-grade, security-hardened deployment for OpenClaw, an AI agent
# platform. All values are parameterized via environment variables with
# sensible defaults. See .env.example for configuration options.
#
# Security Architecture:
#
#  ┌─────────────────────────────────────────────────────────────────────┐
#  │                      CLOUDFLARE TUNNEL (Optional)                   │
#  │                     Profile: "tunnel" (opt-in)                      │
#  │                        cloudflared ←→ Tunnel                        │
#  └────────────────────────────┬────────────────────────────────────────┘
#                               │
#  ┌────────────────────────────▼────────────────────────────────────────┐
#  │                       opendeclawed-gateway                         │
#  │  (REST API, WebSocket, health checks)                              │
#  │  Networks: opendeclawed-internal + opendeclawed-egress             │
#  │  Hardening: drop all caps, no_new_privileges, user 1000:1000       │
#  │  Healthcheck: curl -sf http://127.0.0.1:18789/health              │
#  └────────────────────────────┬────────────────────────────────────────┘
#                               │
#  ┌────────────────────────────▼────────────────────────────────────────┐
#  │                       opendeclawed-litellm                         │
#  │  OpenAI-compatible proxy — single /v1 endpoint for all backends    │
#  │  Routes chat → llama-chat / MLX / Ollama / cloud                   │
#  │  Routes embed → llama-embed / MLX / any embedding endpoint         │
#  │  Internal-only network, all LLM call logs visible in Dozzle        │
#  └────────────────────────────┬────────────────────────────────────────┘
#                               │
#       ┌───────────────────────┴───────────────────────┐
#       │                                               │
#  ┌────▼──────────────────┐               ┌──────────▼────┐
#  │   llama-embed         │               │   llama-chat  │
#  │ (embedding model)     │               │ (chat model)  │
#  │ Internal-only network │               │ Internal-only │
#  │ cap: drop ALL         │               │ cap: drop ALL │
#  └───────────────────────┘               └───────────────┘
#
#  ┌─────────────────────────────────────────────────────────────────────┐
#  │                       EGRESS FIREWALL (Init)                        │
#  │  DOCKER-USER iptables chain: DROP RFC1918, enforces egress control │
#  │  Allows: established/related, DNS, inter-container, Docker bridge  │
#  │  Blocks: 10.0.0.0/8, 192.168.0.0/16, 169.254.0.0/16, multicast    │
#  └─────────────────────────────────────────────────────────────────────┘
#
# NETWORK ISOLATION:
#  - opendeclawed-internal: bridge, internal=true (no internet access)
#  - opendeclawed-egress: bridge, allows outbound via egress firewall rules
#  - Services drop ALL Linux capabilities, run as unprivileged user 65534
#  - Read-only filesystems with minimal tmpfs mounts where needed
#
# PROFILES:
#  - Default: Local-only mode (gateway on 127.0.0.1 via port mapping)
#  - "tunnel": Enable Cloudflare tunnel ingress (cloudflared service)
#  - "tailscale": Enable Tailscale mesh VPN ingress (WireGuard-based)
#  - "monitor": Enable Watchtower + Dozzle (auto-update + log viewer)
#  - "cli": Enable openclaw-cli for onboarding/debugging (interactive)
#
# USAGE:
#  # Local mode (no Cloudflare tunnel):
#  docker-compose up -d
#
#  # With Cloudflare tunnel (requires CLOUDFLARE_TOKEN):
#  docker-compose --profile tunnel up -d
#
#  # With monitoring (Watchtower + Dozzle logs):
#  docker-compose --profile monitor up -d
#
#  # Full stack (tunnel + monitoring):
#  docker-compose --profile tunnel --profile monitor up -d
#
#  # With CLI access (for onboarding):
#  docker-compose --profile cli up -d openclaw-cli
#
# ============================================================================

services:
  # ─────────────────────────────────────────────────────────────────────
  # EGRESS FIREWALL: Kernel-level egress control via iptables
  # ─────────────────────────────────────────────────────────────────────
  # Runs once at initialization to install DOCKER-USER rules that prevent
  # containers from reaching RFC1918 private networks and other restricted
  # ranges. This is the foundational security layer for all outbound traffic.
  #
  # Rules installed:
  #  - DROP to 10.0.0.0/8 (private)
  #  - DROP to 192.168.0.0/16 (private)
  #  - DROP to 169.254.0.0/16 (link-local)
  #  - DROP to 224.0.0.0/4 (multicast)
  #  - DROP to 240.0.0.0/4 (reserved)
  #  - DROP to gateway IP (prevents container→host bypass)
  #  - ACCEPT established/related connections (stateful)
  #  - ACCEPT DNS on :53 (required for container operation)
  #  - ACCEPT to openclaw-egress network (inter-container)
  #  - ACCEPT to docker0 bridge (Docker infrastructure)
  # SECURITY NOTE: This container runs as a PERSISTENT SIDECAR (not init).
  # It re-checks and re-installs rules every 60s, surviving Docker daemon
  # restarts. If Docker restarts, containers come back via restart: unless-stopped
  # but DOCKER-USER rules are flushed. The loop catches this within 60s.
  egress-firewall:
    image: "${ALPINE_IMAGE:-alpine:3.19}"
    container_name: opendeclawed-egress-firewall
    network_mode: host
    cap_add:
      - NET_ADMIN
      - NET_RAW
    cap_drop:
      - ALL
    # NOTE: no-new-privileges, read_only, and non-root user are intentionally
    # omitted — iptables requires root and apk install writes to the filesystem.
    # Mitigated by: Alpine image pinned by tag, Trivy scanning, minimal surface.
    security_opt:
      - no-new-privileges:true
    ipc: private
    restart: unless-stopped
    entrypoint: |
      /bin/sh -c '
        apk add --no-cache iptables ip6tables

        GATEWAY_IP="$${GATEWAY_IP:-172.17.0.1}"
        EGRESS_SUBNET="$${EGRESS_SUBNET:-172.28.0.0/24}"
        INTERNAL_SUBNET="$${INTERNAL_SUBNET:-172.27.0.0/24}"

        install_rules() {
            # Check if our rules are already present (idempotent)
            if iptables -C DOCKER-USER -d 10.0.0.0/8 -j DROP 2>/dev/null; then
                return 0
            fi

            echo "[$(date -u +%H:%M:%S)] Installing egress firewall rules..."

            iptables -N DOCKER-USER 2>/dev/null || true
            ip6tables -N DOCKER-USER 2>/dev/null || true
            iptables -F DOCKER-USER 2>/dev/null || true
            ip6tables -F DOCKER-USER 2>/dev/null || true

            # STATEFUL: Accept established/related
            iptables -I DOCKER-USER 1 -m state --state ESTABLISHED,RELATED -j ACCEPT
            ip6tables -I DOCKER-USER 1 -m state --state ESTABLISHED,RELATED -j ACCEPT

            # ALLOW DNS
            iptables -I DOCKER-USER 2 -p udp --dport 53 -j ACCEPT
            iptables -I DOCKER-USER 3 -p tcp --dport 53 -j ACCEPT
            ip6tables -I DOCKER-USER 2 -p udp --dport 53 -j ACCEPT
            ip6tables -I DOCKER-USER 3 -p tcp --dport 53 -j ACCEPT

            # ALLOW inter-container (openclaw-egress)
            iptables -I DOCKER-USER 4 -d $EGRESS_SUBNET -j ACCEPT
            iptables -I DOCKER-USER 5 -s $EGRESS_SUBNET -j ACCEPT

            # ALLOW inter-container (openclaw-internal)
            iptables -I DOCKER-USER 6 -d $INTERNAL_SUBNET -j ACCEPT
            iptables -I DOCKER-USER 7 -s $INTERNAL_SUBNET -j ACCEPT

            # ALLOW Docker bridge
            iptables -I DOCKER-USER 8 -d 172.17.0.0/16 -j ACCEPT
            iptables -I DOCKER-USER 9 -s 172.17.0.0/16 -j ACCEPT

            # DROP RFC1918
            iptables -A DOCKER-USER -d 10.0.0.0/8 -j DROP
            iptables -A DOCKER-USER -d 192.168.0.0/16 -j DROP
            iptables -A DOCKER-USER -d 172.16.0.0/12 -j DROP

            # DROP link-local, multicast, reserved, gateway
            iptables -A DOCKER-USER -d 169.254.0.0/16 -j DROP
            iptables -A DOCKER-USER -d 224.0.0.0/4 -j DROP
            iptables -A DOCKER-USER -d 240.0.0.0/4 -j DROP
            iptables -A DOCKER-USER -d $GATEWAY_IP -j DROP

            # IPv6
            ip6tables -A DOCKER-USER -d fc00::/7 -j DROP
            ip6tables -A DOCKER-USER -d fe80::/10 -j DROP
            ip6tables -A DOCKER-USER -d ff00::/8 -j DROP

            echo "[$(date -u +%H:%M:%S)] Egress firewall rules installed"
        }

        # Install immediately, then re-check every 60s
        install_rules
        while true; do
            sleep 60
            install_rules
        done
      '
    deploy:
      resources:
        limits:
          cpus: "0.1"
          memory: "32m"
          pids: 32
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3

  # ─────────────────────────────────────────────────────────────────────
  # LLAMA-EMBED: Embedding model server (internal network only)
  # ─────────────────────────────────────────────────────────────────────
  # Runs llama.cpp embedding server. Isolated to internal network
  # (no internet access). Provides EMBED_PORT:${EMBED_PORT:-8090} for
  # openclaw-gateway to call.
  #
  # Security posture:
  #  - Network: openclaw-internal only (no internet)
  #  - Filesystem: read-only + minimal tmpfs for inference cache
  #  - Privileges: user 65534:65534 (nobody:nogroup), no_new_privileges
  #  - Capabilities: ALL dropped (no special kernel access)
  #  - IPC: private (no shared memory)
  #  - PID: private (isolated process namespace)
  llama-embed:
    image: "${LLAMA_IMAGE:-ghcr.io/ggml-org/llama.cpp:server}"
    # The official server image is amd64-only (arm64 CI broken since Apr 2025).
    # OrbStack uses Rosetta 2 for transparent emulation on Apple Silicon.
    platform: "${LLAMA_PLATFORM:-linux/amd64}"
    container_name: opendeclawed-llama-embed
    profiles: [llama]
    command: >-
      --model /llama-models/${EMBED_MODEL_FILE:-nomic-embed-text-v1.5.f16.gguf}
      --host 0.0.0.0
      --threads ${LLAMA_THREADS:-4}
      --ctx-size ${EMBED_CTX:-2048}
      --port ${EMBED_PORT:-8090}
      --embeddings
    networks:
      openclaw-internal: {}
    volumes:
      - llama-models:/llama-models:ro
    tmpfs:
      - /tmp:size=1g,noexec,nosuid,nodev
      - /run:size=256m,noexec,nosuid,nodev
    environment:
      - OMP_NUM_THREADS=${LLAMA_THREADS:-4}
    deploy:
      resources:
        limits:
          cpus: "${LLAMA_EMBED_CPUS:-2}"
          memory: "${LLAMA_EMBED_MEM:-2g}"
          pids: 256
        reservations:
          cpus: "${LLAMA_EMBED_CPUS_RESERVE:-1}"
          memory: "${LLAMA_EMBED_MEM_RESERVE:-1g}"
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    user: "65534:65534"
    ipc: private
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://127.0.0.1:${EMBED_PORT:-8090}/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 60s
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
    restart: unless-stopped
    depends_on:
      egress-firewall:
        condition: service_started

  # ─────────────────────────────────────────────────────────────────────
  # LLAMA-CHAT: Chat model server (internal network only)
  # ─────────────────────────────────────────────────────────────────────
  # Runs llama.cpp chat completion server. Isolated to internal network.
  # Provides CHAT_PORT:${CHAT_PORT:-8091} for openclaw-gateway.
  #
  # Security: Same as llama-embed (see above).
  llama-chat:
    image: "${LLAMA_IMAGE:-ghcr.io/ggml-org/llama.cpp:server}"
    platform: "${LLAMA_PLATFORM:-linux/amd64}"
    container_name: opendeclawed-llama-chat
    profiles: [llama]
    command: >-
      --model /llama-models/${CHAT_MODEL_FILE:-mistral-7b-instruct-v0.2.Q6_K.gguf}
      --host 0.0.0.0
      --threads ${LLAMA_THREADS:-4}
      --ctx-size ${CHAT_CTX:-4096}
      --port ${CHAT_PORT:-8091}
      --n-gpu-layers ${LLAMA_GPU_LAYERS:-0}
    networks:
      openclaw-internal: {}
    volumes:
      - llama-models:/llama-models:ro
    tmpfs:
      - /tmp:size=2g,noexec,nosuid,nodev
      - /run:size=256m,noexec,nosuid,nodev
    environment:
      - OMP_NUM_THREADS=${LLAMA_THREADS:-4}
    deploy:
      resources:
        limits:
          cpus: "${LLAMA_CHAT_CPUS:-3}"
          memory: "${LLAMA_CHAT_MEM:-4g}"
          pids: 256
        reservations:
          cpus: "${LLAMA_CHAT_CPUS_RESERVE:-2}"
          memory: "${LLAMA_CHAT_MEM_RESERVE:-2g}"
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    user: "65534:65534"
    ipc: private
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://127.0.0.1:${CHAT_PORT:-8091}/health"]
      interval: 15s
      timeout: 5s
      retries: 5
      start_period: 60s
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
    restart: unless-stopped
    depends_on:
      egress-firewall:
        condition: service_started

  # ─────────────────────────────────────────────────────────────────────
  # LITELLM: OpenAI-compatible LLM proxy/router
  # ─────────────────────────────────────────────────────────────────────
  # Presents a single /v1/chat/completions and /v1/embeddings endpoint
  # to OpenClaw, abstracting the actual backend (llama.cpp containers,
  # MLX on host, Ollama, vLLM, etc.). Backend routing is defined in
  # litellm_config.yaml — swap backends without changing openclaw.json.
  #
  # Profile: always-on (core infrastructure)
  #
  # Security:
  #  - Internal network only (no internet access)
  #  - read-only root, all caps dropped, no_new_privileges
  #  - Master key set to internal-only token (not exposed externally)
  #  - No database (stateless proxy — logs to stdout for Dozzle)
  litellm:
    image: "${LITELLM_IMAGE:-ghcr.io/berriai/litellm:main-stable}"
    container_name: opendeclawed-litellm
    command: ["--config", "/app/config.yaml", "--port", "${LITELLM_PORT:-4000}"]
    networks:
      openclaw-internal: {}
    volumes:
      - ${LITELLM_CONFIG:-./litellm_config.yaml}:/app/config.yaml:ro
    environment:
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-opendeclawed-internal}
      - LITELLM_LOG_LEVEL=${LOG_LEVEL:-info}
    deploy:
      resources:
        limits:
          cpus: "${LITELLM_CPUS:-1}"
          memory: "${LITELLM_MEM:-512m}"
          pids: 256
        reservations:
          cpus: "0.25"
          memory: "128m"
    read_only: true
    tmpfs:
      - /tmp:size=256m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    user: "65534:65534"
    ipc: private
    healthcheck:
      test: ["CMD", "python", "-c", "import urllib.request; urllib.request.urlopen('http://127.0.0.1:${LITELLM_PORT:-4000}/health/liveliness')"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 20s
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 5
    restart: unless-stopped
    depends_on:
      egress-firewall:
        condition: service_started

  # ─────────────────────────────────────────────────────────────────────
  # OPENCLAW-GATEWAY: API server orchestrating LLM backends
  # ─────────────────────────────────────────────────────────────────────
  # Central OpenClaw gateway service. Hosts REST API, WebSocket, and health
  # checks. Bridges internal (LiteLLM proxy) and egress (outbound) networks.
  #
  # Networking:
  #  - openclaw-internal: Can reach litellm:4000 (LLM router)
  #  - openclaw-egress: Subject to egress firewall rules (no RFC1918)
  #  - Ports bound to 127.0.0.1 by default (local-only); tunnel mode disables
  #
  # Healthcheck:
  #  - curl -sf http://127.0.0.1:${GATEWAY_PORT:-18789}/health
  #  - Fails if gateway crashes or becomes unresponsive
  #  - Required for cloudflared dependency: service_healthy
  #
  # Security:
  #  - NET_BIND_SERVICE: Only cap needed (bind to ports <1024 if configured)
  #  - read-only filesystem; persistent state via bind mounts to ~/.openclaw
  #  - Container sees ONLY config + workspace directories, no other host paths
  #  - no_new_privileges, drop all capabilities, unprivileged user
  #  - ipc: private (isolated)
  openclaw-gateway:
    image: "${OPENCLAW_IMAGE:-openclaw:local}"
    container_name: opendeclawed-gateway
    # Upstream CMD: node openclaw.mjs gateway --allow-unconfigured
    # Built image uses dist/index.js. We add --bind lan for container networking.
    command: ["node", "dist/index.js", "gateway", "--allow-unconfigured", "--bind", "lan", "--port", "${GATEWAY_PORT:-18789}"]
    init: true
    # Route DNS through blocky for threat filtering + DoH privacy.
    # Docker dns: requires an IP, not a hostname. Blocky is pinned to
    # 172.27.0.53 on the internal network (see blocky service below).
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    ports:
      # Local mode: bind to 127.0.0.1 (default)
      # Tunnel mode: no port binding (cloudflared proxies)
      - "127.0.0.1:${GATEWAY_PORT:-18789}:${GATEWAY_PORT:-18789}"
    volumes:
      # Persistent config — bind mount to host ~/.openclaw (survives daemon restarts).
      # Matches upstream: /home/node/.openclaw is the default config path.
      - ${OPENCLAW_CONFIG_DIR:-~/.openclaw}:/home/node/.openclaw:rw
      - ${OPENCLAW_WORKSPACE_DIR:-~/.openclaw/workspace}:/home/node/.openclaw/workspace:rw
    tmpfs:
      - /tmp:size=2g,noexec,nosuid,nodev
      - /run:size=512m,noexec,nosuid,nodev
    environment:
      - HOME=/home/node
      - OPENCLAW_GATEWAY_TOKEN=${OPENCLAW_GATEWAY_TOKEN:-}
      - LITELLM_HOST=litellm
      - LITELLM_PORT=${LITELLM_PORT:-4000}
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY:-sk-opendeclawed-internal}
      - GATEWAY_PORT=${GATEWAY_PORT:-18789}
      - LOG_LEVEL=${LOG_LEVEL:-info}
    deploy:
      resources:
        limits:
          cpus: "${GATEWAY_CPUS:-2}"
          memory: "${GATEWAY_MEM:-4g}"
          pids: 256
        reservations:
          cpus: "${GATEWAY_CPUS_RESERVE:-1}"
          memory: "${GATEWAY_MEM_RESERVE:-2g}"
    # Note: read_only omitted — Node.js app writes temp files to /app/.cache, /tmp, etc.
    # Hardening still applied via: no-new-privileges, cap_drop ALL, non-root user, tmpfs.
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    # Upstream image sets USER node (uid 1000). Must match for /app file access.
    user: "1000:1000"
    ipc: private
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://127.0.0.1:${GATEWAY_PORT:-18789}/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 5
    restart: unless-stopped
    labels:
      - "com.centurylinklabs.watchtower.enable=true"
    depends_on:
      egress-firewall:
        condition: service_started
      litellm:
        condition: service_healthy

  # ─────────────────────────────────────────────────────────────────────
  # CLOUDFLARED: Cloudflare Tunnel ingress (optional, profile: tunnel)
  # ─────────────────────────────────────────────────────────────────────
  # Establishes a secure tunnel to Cloudflare, making openclaw-gateway
  # accessible via Cloudflare's global network without exposing host ports.
  #
  # Profile: "tunnel"
  #  - Enable with: docker-compose --profile tunnel up -d
  #  - Requires CLOUDFLARE_TOKEN env var
  #
  # Security:
  #  - Depends on: egress-firewall (init), openclaw-gateway (healthy)
  #  - read-only filesystem
  #  - NO capabilities at all (cap_drop: [ALL], no cap_add)
  #  - user 65534:65534 (nobody:nogroup)
  #  - ipc: private (isolated)
  #  - Service will not start until gateway is healthy
  #
  # Configuration:
  #  - CLOUDFLARE_TOKEN: Required for tunnel auth (get from Cloudflare dashboard)
  #  - CLOUDFLARE_TUNNEL_NAME: Tunnel ID or name
  #  - CLOUDFLARE_TUNNEL_ROUTE: Hostname to route (e.g., openclaw.example.com)
  cloudflared:
    image: "${CLOUDFLARED_IMAGE:-cloudflare/cloudflared:2024.1.1}"
    container_name: opendeclawed-cloudflared
    profiles:
      - tunnel
    # NOTE: Token-based tunnels use Cloudflare dashboard ingress rules, which
    # take precedence over this local config. For token tunnels, configure
    # routes in Cloudflare Zero Trust dashboard under Networks > Tunnels >
    # Public Hostname. The local config below serves as documentation and
    # works for locally-managed (credentials-file) tunnels.
    entrypoint: |
      /bin/sh -c '
        mkdir -p /etc/cloudflared
        cat > /etc/cloudflared/config.yml << CFGEOF
      ingress:
        - hostname: "${CLOUDFLARE_TUNNEL_ROUTE:-openclaw.example.com}"
          service: "http://openclaw-gateway:${GATEWAY_PORT:-18789}"
        - service: http_status:404
      CFGEOF
        exec cloudflared tunnel --config /etc/cloudflared/config.yml run --token "${CLOUDFLARE_TOKEN}"
      '
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
    read_only: true
    tmpfs:
      - /etc/cloudflared:size=1m,noexec,nosuid,nodev
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    user: "65534:65534"
    ipc: private
    environment:
      - CLOUDFLARE_TOKEN=${CLOUDFLARE_TOKEN:-}
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
    restart: unless-stopped
    depends_on:
      egress-firewall:
        condition: service_started
      openclaw-gateway:
        condition: service_healthy

  # ─────────────────────────────────────────────────────────────────────
  # TAILSCALE: Zero-config mesh VPN ingress (optional, profile: tailscale)
  # ─────────────────────────────────────────────────────────────────────
  # Exposes openclaw-gateway via your Tailscale tailnet. Devices on the
  # same tailnet can reach the gateway at https://<hostname>.<tailnet>.ts.net.
  # Uses Tailscale Serve to proxy HTTPS → gateway HTTP internally.
  #
  # Profile: "tailscale"
  #  - Enable with: docker compose --profile tailscale up -d
  #  - Requires TS_AUTHKEY env var (one-time or reusable auth key)
  #
  # Security:
  #  - Traffic encrypted end-to-end via WireGuard (Tailscale)
  #  - Access controlled by Tailscale ACLs (your admin console)
  #  - No public ports exposed; only tailnet members can connect
  #  - Persistent state volume avoids re-auth on restart
  #  - cap_add: NET_ADMIN + NET_RAW (required for WireGuard tunnel)
  #  - Read-only root, all other caps dropped
  #
  # Configuration:
  #  - TS_AUTHKEY: Auth key from https://login.tailscale.com/admin/settings/keys
  #  - TS_HOSTNAME: Machine name on your tailnet (default: "openclaw")
  #  - TS_EXTRA_ARGS: Additional tailscaled flags (optional)
  tailscale:
    image: "${TAILSCALE_IMAGE:-tailscale/tailscale:v1.76.6}"
    container_name: opendeclawed-tailscale
    profiles:
      - tailscale
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    dns:
      - "${BLOCKY_IP:-172.27.0.53}"
    volumes:
      - tailscale-state:/var/lib/tailscale
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
    environment:
      - TS_AUTHKEY=${TS_AUTHKEY:-}
      - TS_HOSTNAME=${TS_HOSTNAME:-openclaw}
      - TS_STATE_DIR=/var/lib/tailscale
      - TS_SERVE_CONFIG=/config/serve.json
      - TS_USERSPACE=true
      - TS_EXTRA_ARGS=${TS_EXTRA_ARGS:-}
    # Tailscale Serve config: proxy HTTPS on :443 → gateway HTTP
    entrypoint: |
      /bin/sh -c '
        mkdir -p /config
        cat > /config/serve.json << SERVECFG
        {
          "TCP": {
            "443": {
              "HTTPS": true
            }
          },
          "Web": {
            "$${TS_HOSTNAME:-openclaw}.$${TS_CERT_DOMAIN:-ts.net}:443": {
              "Handlers": {
                "/": {
                  "Proxy": "http://openclaw-gateway:${GATEWAY_PORT:-18789}"
                }
              }
            }
          }
        }
      SERVECFG
        exec /usr/local/bin/containerboot
      '
    deploy:
      resources:
        limits:
          cpus: "${TAILSCALE_CPUS:-0.5}"
          memory: "${TAILSCALE_MEM:-128m}"
          pids: 128
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_ADMIN
      - NET_RAW
    # NOTE: user directive omitted — Tailscale needs root for WireGuard tunnel setup.
    # Mitigated by: read_only, no-new-privileges, cap_drop ALL + selective cap_add.
    ipc: private
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
    restart: unless-stopped
    depends_on:
      egress-firewall:
        condition: service_started
      openclaw-gateway:
        condition: service_healthy
    labels:
      - "com.centurylinklabs.watchtower.enable=false"

  # ─────────────────────────────────────────────────────────────────────
  # DOCKER-SOCKET-PROXY: Least-privilege Docker API access for Watchtower
  # ─────────────────────────────────────────────────────────────────────
  # Instead of giving Watchtower direct access to /var/run/docker.sock
  # (which is equivalent to root on the host), we proxy only the specific
  # Docker API endpoints Watchtower needs: container list, image pull,
  # and container restart. Everything else is denied.
  #
  # Security:
  #  - Only exposes GET/POST to containers, images, version, events
  #  - Denies: exec, volumes, networks, secrets, swarm, build, commit
  #  - Internal network only (no internet access)
  #  - cap_drop ALL, no_new_privileges
  docker-socket-proxy:
    image: "${SOCKET_PROXY_IMAGE:-tecnativa/docker-socket-proxy:0.2.0}"
    container_name: opendeclawed-socket-proxy
    profiles:
      - monitor
    networks:
      openclaw-internal: {}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    environment:
      # Allow only what Watchtower needs
      - CONTAINERS=1
      - IMAGES=1
      - VERSION=1
      - EVENTS=1
      - POST=1
      # Deny everything else
      - AUTH=0
      - BUILD=0
      - COMMIT=0
      - CONFIGS=0
      - DISTRIBUTION=0
      - EXEC=0
      - GRPC=0
      - INFO=1
      - NETWORKS=0
      - NODES=0
      - PLUGINS=0
      - SECRETS=0
      - SERVICES=0
      - SESSION=0
      - SWARM=0
      - SYSTEM=0
      - TASKS=0
      - VOLUMES=0
    deploy:
      resources:
        limits:
          cpus: "0.5"
          memory: "64m"
          pids: 64
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # NOTE: user directive omitted — socket proxy needs root to bind port 2375.
    # NOTE: read_only omitted — entrypoint generates haproxy.cfg from template
    # at /usr/local/etc/haproxy/ on startup. Mitigated by cap_drop ALL +
    # no-new-privileges + Docker socket being the actual security boundary.
    ipc: private
    tmpfs:
      - /tmp:size=16m,noexec,nosuid,nodev
      - /run:size=16m,noexec,nosuid,nodev
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
    restart: unless-stopped
    labels:
      - "com.centurylinklabs.watchtower.enable=false"

  # ─────────────────────────────────────────────────────────────────────
  # BLOCKY: DNS firewall with threat intelligence blocklists
  # ─────────────────────────────────────────────────────────────────────
  # Lightweight DNS proxy that filters queries against known-malicious
  # domains (malware C2, phishing, cryptomining, trackers). Upstream
  # resolvers use DNS-over-HTTPS for privacy. All containers on the
  # egress network use this as their DNS server.
  #
  # Profile: always-on (no profile — runs with default stack)
  #
  # Security:
  #  - Blocks known-malicious domains at DNS layer
  #  - Upstream via DoH (DNS-over-HTTPS) — no plaintext DNS to ISP
  #  - Internal network only for serving; egress for upstream DoH
  #  - cap_drop ALL, no_new_privileges, read-only root
  #  - Provides defense against DNS exfiltration tunneling via rate limits
  blocky:
    image: "${BLOCKY_IMAGE:-spx01/blocky:v0.24}"
    container_name: opendeclawed-blocky
    command: ["--config", "/app/config/config.yml"]
    networks:
      openclaw-internal:
        ipv4_address: "${BLOCKY_IP:-172.27.0.53}"
      openclaw-egress: {}
    volumes:
      - blocky-config:/app/config:ro
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
    deploy:
      resources:
        limits:
          cpus: "${BLOCKY_CPUS:-0.5}"
          memory: "${BLOCKY_MEM:-64m}"
          pids: 64
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_BIND_SERVICE
    user: "65534:65534"
    ipc: private
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
    restart: unless-stopped
    depends_on:
      egress-firewall:
        condition: service_started
    labels:
      - "com.centurylinklabs.watchtower.enable=true"

  # ─────────────────────────────────────────────────────────────────────
  # WATCHTOWER: Automatic container image updates (optional, profile: monitor)
  # ─────────────────────────────────────────────────────────────────────
  # Monitors running containers and updates them when new images are
  # available. Label-controlled: only updates containers with
  # com.centurylinklabs.watchtower.enable=true.
  #
  # Profile: "monitor"
  #  - Enable with: docker-compose --profile monitor up -d
  #
  # Security:
  #  - Requires /var/run/docker.sock (read-only where possible)
  #  - cap_drop ALL, no_new_privileges
  #  - Label-gated: won't touch unlabeled containers
  #  - Poll interval configurable (default 24h)
  #  - Read-only root filesystem
  watchtower:
    image: "${WATCHTOWER_IMAGE:-containrrr/watchtower:1.7.1}"
    container_name: opendeclawed-watchtower
    profiles:
      - monitor
    networks:
      openclaw-internal: {}
    # SECURITY: No direct Docker socket access. Uses docker-socket-proxy
    # which only exposes container/image API endpoints.
    environment:
      - DOCKER_HOST=tcp://docker-socket-proxy:2375
      - WATCHTOWER_LABEL_ENABLE=true
      - WATCHTOWER_POLL_INTERVAL=${WATCHTOWER_POLL_SECONDS:-86400}
      - WATCHTOWER_CLEANUP=true
      - WATCHTOWER_INCLUDE_STOPPED=false
      - WATCHTOWER_NO_RESTART=false
      - WATCHTOWER_ROLLING_RESTART=true
      - WATCHTOWER_NOTIFICATIONS=${WATCHTOWER_NOTIFICATIONS:-}
      - WATCHTOWER_NOTIFICATION_URL=${WATCHTOWER_NOTIFICATION_URL:-}
    tmpfs:
      - /tmp:size=64m,noexec,nosuid,nodev
    deploy:
      resources:
        limits:
          cpus: "${WATCHTOWER_CPUS:-0.5}"
          memory: "${WATCHTOWER_MEM:-128m}"
          pids: 64
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    # NOTE: user directive omitted — Watchtower image runs as root by default
    # and needs it for Docker API interactions. Mitigated by socket proxy.
    ipc: private
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
    restart: unless-stopped
    depends_on:
      docker-socket-proxy:
        condition: service_started
    labels:
      - "com.centurylinklabs.watchtower.enable=false"

  # ─────────────────────────────────────────────────────────────────────
  # DOZZLE: Real-time container log viewer (monitor profile)
  # ─────────────────────────────────────────────────────────────────────
  # Lightweight web UI for viewing, filtering, and searching container
  # logs in real time. Reads Docker's native json-file logs — no log
  # driver changes or sidecars required. Read-only Docker socket access.
  #
  # Profile: "monitor" (same as Watchtower)
  #
  # Security:
  #  - Docker API via socket-proxy (no direct socket mount — prevents secret leakage via docker inspect)
  #  - DOZZLE_FILTER: only shows openclaw-* containers
  #  - DOZZLE_NO_ANALYTICS: disables telemetry to Dozzle maintainers
  #  - Port bound to 127.0.0.1 (local access only)
  #  - read-only root, all caps dropped, no_new_privileges
  dozzle:
    image: "${DOZZLE_IMAGE:-amir20/dozzle:v8.8.2}"
    container_name: opendeclawed-dozzle
    profiles:
      - monitor
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    # SECURITY: No direct Docker socket. Uses socket-proxy (same as Watchtower).
    # Dozzle only needs container list + events for log streaming.
    environment:
      - DOZZLE_REMOTE_HOST=tcp://docker-socket-proxy:2375
      - DOZZLE_ADDR=:${DOZZLE_PORT:-5005}
      - DOZZLE_LEVEL=info
      - DOZZLE_FILTER=name=opendeclawed-*
      - DOZZLE_NO_ANALYTICS=true
    # NOTE: dns directive omitted — Dozzle needs Docker's embedded DNS (127.0.0.11)
    # to resolve docker-socket-proxy. Blocky DNS would break service discovery.
    tmpfs:
      - /tmp:size=32m,noexec,nosuid,nodev
    ports:
      - "127.0.0.1:${DOZZLE_PORT:-5005}:${DOZZLE_PORT:-5005}"
    deploy:
      resources:
        limits:
          cpus: "${DOZZLE_CPUS:-0.25}"
          memory: "${DOZZLE_MEM:-64m}"
          pids: 64
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    user: "65534:65534"
    ipc: private
    logging:
      driver: json-file
      options:
        max-size: 5m
        max-file: 2
    restart: unless-stopped
    depends_on:
      docker-socket-proxy:
        condition: service_started

  # ─────────────────────────────────────────────────────────────────────
  # OPENCLAW-CLI: Interactive CLI for onboarding and debugging
  # ─────────────────────────────────────────────────────────────────────
  # Interactive shell for running openclaw commands, onboarding workflows,
  # and debugging. Shares networks with gateway for API access.
  #
  # Profile: "cli"
  #  - Enable with: docker-compose --profile cli run openclaw-cli bash
  #  - Interactive (stdin_open: true, tty: true)
  #  - Grants NET_RAW for ping/traceroute if needed
  #
  # Security:
  #  - Runs as node user (1000:1000), matching upstream image
  #  - cap_add: [NET_RAW] for network diagnostics
  #  - Other caps still dropped, no_new_privileges, ipc isolated
  #  - Read-write home volume for credentials/config
  openclaw-cli:
    image: "${OPENCLAW_IMAGE:-openclaw:local}"
    container_name: opendeclawed-cli
    entrypoint: ["node", "dist/index.js"]
    init: true
    profiles:
      - cli
    stdin_open: true
    tty: true
    networks:
      openclaw-internal: {}
      openclaw-egress: {}
    volumes:
      - ${OPENCLAW_CONFIG_DIR:-~/.openclaw}:/home/node/.openclaw:rw
      - ${OPENCLAW_WORKSPACE_DIR:-~/.openclaw/workspace}:/home/node/.openclaw/workspace:rw
    tmpfs:
      - /tmp:size=1g,noexec,nosuid,nodev
      - /run:size=256m,noexec,nosuid,nodev
    environment:
      - HOME=/home/node
      - TERM=xterm-256color
      - BROWSER=echo
      - OPENCLAW_GATEWAY_TOKEN=${OPENCLAW_GATEWAY_TOKEN:-}
      - LITELLM_HOST=litellm
      - LITELLM_PORT=${LITELLM_PORT:-4000}
      - LITELLM_API_KEY=${LITELLM_MASTER_KEY:-sk-opendeclawed-internal}
      - GATEWAY_HOST=openclaw-gateway
      - GATEWAY_PORT=${GATEWAY_PORT:-18789}
      - LOG_LEVEL=${LOG_LEVEL:-debug}
    deploy:
      resources:
        limits:
          cpus: "1"
          memory: "2g"
          pids: 256
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - NET_RAW
    user: "1000:1000"
    ipc: private
    logging:
      driver: json-file
      options:
        max-size: 10m
        max-file: 3
    depends_on:
      egress-firewall:
        condition: service_started
      openclaw-gateway:
        condition: service_started

networks:
  # ─────────────────────────────────────────────────────────────────────
  # OPENCLAW-INTERNAL: LLM service mesh (no internet access)
  # ─────────────────────────────────────────────────────────────────────
  # Bridge network for litellm, llama-embed, llama-chat, and openclaw-gateway
  # to communicate. Marked internal: true so containers cannot reach the host
  # or internet directly. Subject to egress firewall rules (allows
  # inter-container within 172.27.0.0/16).
  openclaw-internal:
    driver: bridge
    internal: true
    ipam:
      config:
        - subnet: "${INTERNAL_SUBNET:-172.27.0.0/24}"

  # ─────────────────────────────────────────────────────────────────────
  # OPENCLAW-EGRESS: Egress-controlled network (outbound rules apply)
  # ─────────────────────────────────────────────────────────────────────
  # Bridge network for openclaw-gateway and cloudflared. Subject to strict
  # egress firewall rules: allows DNS, established/related, and inter-
  # container traffic; blocks RFC1918 private ranges and reserved IPs.
  # Used for cloudflared (tunnel) and controlled external API calls.
  openclaw-egress:
    driver: bridge
    ipam:
      config:
        - subnet: "${EGRESS_SUBNET:-172.28.0.0/24}"

volumes:
  # ─────────────────────────────────────────────────────────────────────
  # BLOCKY-CONFIG: DNS firewall configuration
  # ─────────────────────────────────────────────────────────────────────
  blocky-config:
    driver: local

  # ─────────────────────────────────────────────────────────────────────
  # TAILSCALE-STATE: Persistent Tailscale auth state
  # ─────────────────────────────────────────────────────────────────────
  # Stores WireGuard keys and auth state. Persisting this avoids
  # re-authentication on container restart.
  tailscale-state:
    driver: local


  # ─────────────────────────────────────────────────────────────────────
  # OPENCLAW HOME: Bind-mounted from host (no volume definition needed)
  # ─────────────────────────────────────────────────────────────────────
  # The gateway and CLI containers bind-mount two host directories:
  #   ${OPENCLAW_CONFIG_DIR:-~/.openclaw}     → /home/openclaw/config
  #   ${OPENCLAW_WORKSPACE_DIR:-~/.openclaw/workspace} → /home/openclaw/workspace
  #
  # These are the ONLY host paths accessible to the gateway container.
  # Docker enforces this — containers cannot mount paths not declared
  # in their compose volumes section (and we block Docker socket access
  # via the socket proxy, preventing runtime volume creation).
  #
  # Host-side permissions (set by setup.sh):
  #   ~/.openclaw/            → 700 (owner only)
  #   ~/.openclaw/openclaw.json → 600
  #   ~/.openclaw/workspace/  → 700

  # ─────────────────────────────────────────────────────────────────────
  # LLAMA-MODELS: Shared model cache (read-only for services)
  # ─────────────────────────────────────────────────────────────────────
  # GGUF quantized model files for llama-embed and llama-chat.
  # Bind-mounted from host at startup. Models must be placed in
  # LLAMA_MODELS_PATH on the host before docker-compose up.
  #
  # Models needed:
  #  - ${EMBED_MODEL_FILE:-nomic-embed-text-v1.5.f16.gguf} (embedding)
  #  - ${CHAT_MODEL_FILE:-mistral-7b-instruct-v0.2.Q6_K.gguf} (chat)
  #
  # Setup:
  #  mkdir -p ${LLAMA_MODELS_PATH:-./models}
  #  # Download models here
  #  docker-compose up -d
  llama-models:
    driver: local
    driver_opts:
      type: none
      o: bind
      device: "${LLAMA_MODELS_PATH:-./models}"
