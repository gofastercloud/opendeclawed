# ============================================================================
# OpenDeclawed - Environment Configuration Template
# ============================================================================
# Copy this file to .env and customize for your deployment.
# All values shown are sensible defaults; adjust for your infrastructure.
#
# See docker-compose.yml for detailed security documentation.
# ============================================================================

# ─────────────────────────────────────────────────────────────────────────
# CONTAINER IMAGES
# ─────────────────────────────────────────────────────────────────────────
# Docker image references. Use specific tags (not 'latest') for production.

# OpenClaw gateway and CLI image
# Default: build locally with "docker build -t openclaw:local ." in the openclaw repo
# Alt pre-built: alpine/openclaw (Docker Hub mirror of ghcr.io/openclaw/openclaw)
OPENCLAW_IMAGE=openclaw:local

# ── LLM BACKEND ────────────────────────────────────────────────────────
# llama.cpp runs as a containerized backend behind the LiteLLM proxy.
# To use MLX, Ollama, or another backend, edit litellm_config.yaml to
# change the api_base URLs (e.g. http://host.docker.internal:8091/v1).
LLAMA_IMAGE=ghcr.io/ggml-org/llama.cpp:server

# Cloudflare tunnel client (for --profile tunnel mode)
CLOUDFLARED_IMAGE=cloudflare/cloudflared:2024.1.1

# Alpine base image (used for egress-firewall init container)
ALPINE_IMAGE=alpine:3.19

# ─────────────────────────────────────────────────────────────────────────
# MODEL FILES (GGUF quantized format)
# ─────────────────────────────────────────────────────────────────────────
# Filenames of model files in the llama-models volume.
# These must exist at ${LLAMA_MODELS_PATH}/filename before starting.
#
# Popular options:
#  Embedding: nomic-embed-text-v1.5.f16.gguf (500MB)
#  Chat:      mistral-7b-instruct-v0.2.Q6_K.gguf (5.8GB)
#  Chat:      neural-chat-7b-v3-3.Q6_K.gguf (5.8GB)
#  Chat:      openchat-3.5-1210.Q6_K.gguf (4.5GB)

EMBED_MODEL_FILE=nomic-embed-text-v1.5.f16.gguf
CHAT_MODEL_FILE=mistral-7b-instruct-v0.2.Q6_K.gguf

# ─────────────────────────────────────────────────────────────────────────
# LLAMA.CPP SERVER CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────
# Parameters controlling model inference behavior.

# Number of threads for CPU inference (set to CPU core count or less)
LLAMA_THREADS=4

# GPU acceleration: number of layers to offload to GPU (0 = CPU only)
LLAMA_GPU_LAYERS=0

# Context window size for embedding model (tokens)
# Higher = more expensive, better long-document support
EMBED_CTX=2048

# Context window size for chat model (tokens)
# Larger contexts enable longer conversations but increase memory usage
CHAT_CTX=4096

# ─────────────────────────────────────────────────────────────────────────
# RESOURCE LIMITS (CPU, Memory)
# ─────────────────────────────────────────────────────────────────────────
# Docker Compose resource constraints (deploy.resources).
# Set limits to prevent container runaway; set reservations for guaranteed QoS.
#
# Format: memory (e.g., "2g", "512m"), CPU (e.g., "2", "0.5")
#
# Memory rough estimates (depends on model quantization):
#  - llama-embed:  1-2g (with context)
#  - llama-chat:   4-8g (with context)
#  - gateway:      2-4g (Python runtime + agent state)
#  - Total:        ~8-14g baseline

# LLAMA-EMBED resource limits
LLAMA_EMBED_CPUS=2
LLAMA_EMBED_MEM=2g
LLAMA_EMBED_CPUS_RESERVE=1
LLAMA_EMBED_MEM_RESERVE=1g

# LLAMA-CHAT resource limits
LLAMA_CHAT_CPUS=3
LLAMA_CHAT_MEM=4g
LLAMA_CHAT_CPUS_RESERVE=2
LLAMA_CHAT_MEM_RESERVE=2g

# OPENCLAW-GATEWAY resource limits
GATEWAY_CPUS=2
GATEWAY_MEM=4g
GATEWAY_CPUS_RESERVE=1
GATEWAY_MEM_RESERVE=2g

# ─────────────────────────────────────────────────────────────────────────
# NETWORK PORTS
# ─────────────────────────────────────────────────────────────────────────
# TCP ports for each service. Ensure no conflicts with host services.
# In local mode (default), all bound to 127.0.0.1 (loopback only).
# In tunnel mode (--profile tunnel), gateway port is not exposed locally.

# Gateway REST API / WebSocket port (localhost:18789 by default)
GATEWAY_PORT=18789

# ── LiteLLM PROXY (LLM router) ───────────────────────────────────────
# All local LLM traffic flows through LiteLLM, which routes to the
# actual backend (llama.cpp, MLX, Ollama, vLLM, etc.) based on
# litellm_config.yaml. Swap backends by editing that YAML — no
# openclaw.json changes needed.
LITELLM_PORT=4000
LITELLM_MASTER_KEY=sk-opendeclawed-internal
LITELLM_IMAGE=ghcr.io/berriai/litellm:main-stable
LITELLM_CPUS=1
LITELLM_MEM=256m

# Path to LiteLLM config (generated by setup.sh, edit to add backends)
LITELLM_CONFIG=./litellm_config.yaml

# ─────────────────────────────────────────────────────────────────────────
# NETWORK CONFIGURATION (SUBNETS)
# ─────────────────────────────────────────────────────────────────────────
# Docker bridge network subnets. Keep these unused on your host to avoid
# conflicts. See docker-compose.yml for network architecture.

# openclaw-internal network (llama servers + gateway, no internet)
INTERNAL_SUBNET=172.27.0.0/24

# openclaw-egress network (gateway + cloudflared, egress-controlled)
EGRESS_SUBNET=172.28.0.0/24

# Blocky DNS server IP on internal network (must be static for dns: directive)
BLOCKY_IP=172.27.0.53

# openclaw-meshnet network (NordVPN meshnet + Caddy proxy, no internet)
MESHNET_SUBNET=172.29.0.0/24

# ─────────────────────────────────────────────────────────────────────────
# EGRESS FIREWALL RULES
# ─────────────────────────────────────────────────────────────────────────
# The egress-firewall service installs DOCKER-USER iptables rules on init.
# These cannot be changed at runtime; you must restart containers to apply.
#
# Default rules:
#  - ACCEPT: established/related (stateful)
#  - ACCEPT: DNS (:53)
#  - ACCEPT: inter-container on openclaw-egress network
#  - ACCEPT: Docker bridge (docker0, 172.17.0.0/16)
#  - DROP: RFC1918 (10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12)
#  - DROP: link-local (169.254.0.0/16)
#  - DROP: multicast (224.0.0.0/4)
#  - DROP: reserved (240.0.0.0/4)
#  - DROP: gateway IP (172.17.0.1, prevents container→host bypass)
#
# To modify these rules, edit docker-compose.yml egress-firewall entrypoint
# and restart: docker-compose down && docker-compose up -d

# Subnet for egress-controlled outbound traffic (can reach external APIs)
# Must match EGRESS_SUBNET above
EGRESS_SUBNET=172.28.0.0/24

# ─────────────────────────────────────────────────────────────────────────
# CLOUDFLARE TUNNEL (optional, requires --profile tunnel)
# ─────────────────────────────────────────────────────────────────────────
# Enable secure public access via Cloudflare's global network without
# exposing host ports. See docker-compose.yml cloudflared service.
#
# Setup:
#  1. Go to https://dash.cloudflare.com/ → Tunnels → Create tunnel
#  2. Name: "openclaw" (or your preference)
#  3. Copy the tunnel token: cloudflare/....
#  4. Set CLOUDFLARE_TOKEN below
#  5. Create a DNS record routing your domain to this tunnel
#  6. Run: docker-compose --profile tunnel up -d

# Cloudflare tunnel authentication token (required to run tunnel profile)
# Get from: https://dash.cloudflare.com/ → Tunnels → <tunnel> → View token
# Format: eyJhIjoiXXXXXXXXXXXXXXXXXXXXXXXX...
CLOUDFLARE_TOKEN=

# Optional: Tunnel name or ID (for debugging/logging)
CLOUDFLARE_TUNNEL_NAME=openclaw

# Optional: Public hostname(s) this tunnel routes (for reference only)
CLOUDFLARE_TUNNEL_ROUTE=openclaw.example.com

# ─────────────────────────────────────────────────────────────────────────
# TAILSCALE MESH VPN (optional, requires --profile tailscale)
# ─────────────────────────────────────────────────────────────────────────
# Expose openclaw-gateway to your Tailscale tailnet via WireGuard.
# Access controlled by Tailscale ACLs — only tailnet members can connect.
#
# Setup:
#  1. Go to https://login.tailscale.com/admin/settings/keys
#  2. Generate an auth key (reusable recommended for unattended restarts)
#  3. Set TS_AUTHKEY below
#  4. Run: docker compose --profile tailscale up -d
#  5. Access at: https://<TS_HOSTNAME>.<your-tailnet>.ts.net

# Tailscale image
TAILSCALE_IMAGE=tailscale/tailscale:latest

# Auth key (required to join your tailnet)
# Format: tskey-auth-XXXX or tskey-client-XXXX
TS_AUTHKEY=

# Machine hostname on your tailnet (default: "openclaw")
TS_HOSTNAME=openclaw

# Additional tailscaled flags (optional, for advanced use)
TS_EXTRA_ARGS=

# Resource limits
TAILSCALE_CPUS=0.5
TAILSCALE_MEM=128m

# ─────────────────────────────────────────────────────────────────────────
# NORDVPN MESHNET (optional, requires --profile meshnet)
# ─────────────────────────────────────────────────────────────────────────
# Expose openclaw-gateway to your NordVPN meshnet peers via peer-to-peer
# WireGuard tunnels. Only devices on your meshnet can connect.
#
# Setup:
#  1. Go to https://my.nordaccount.com → Services → NordVPN → Access Token
#  2. Generate a service token
#  3. Set NORDVPN_TOKEN below
#  4. Run: docker compose --profile meshnet up -d
#  5. On your device: nordvpn meshnet peer connect <hostname>

# NordVPN service token (required to enable meshnet)
NORDVPN_TOKEN=

# Resource limits
NORDVPN_CPUS=0.5
NORDVPN_MEM=256m

# Caddy reverse proxy image (used by meshnet profile for TLS termination)
CADDY_IMAGE=caddy:alpine
CADDY_CPUS=0.25
CADDY_MEM=64m

# ─────────────────────────────────────────────────────────────────────────
# TELEMETRY (opt-in, disabled by default)
# ─────────────────────────────────────────────────────────────────────────
# OpenClaw native telemetry. When enabled, sends anonymous usage metrics
# to help the project. No credentials, prompts, or PII are transmitted.
# Default: false (privacy-first).

TELEMETRY_ENABLED=false

# ─────────────────────────────────────────────────────────────────────────
# OPENCLAW-GATEWAY CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────
# Gateway application settings and logging.

# Logging level: debug, info, warn, error
LOG_LEVEL=info

# ─────────────────────────────────────────────────────────────────────────
# MODEL CACHE PATH (HOST-SIDE)
# ─────────────────────────────────────────────────────────────────────────
# Host directory containing GGUF model files. This is bind-mounted into
# containers at /llama-models (read-only).
#
# Setup:
#  mkdir -p ./models
#  # Download model files into ./models/
#  ls ./models/
#  nomic-embed-text-v1.5.f16.gguf
#  mistral-7b-instruct-v0.2.Q6_K.gguf
#  docker-compose up -d

LLAMA_MODELS_PATH=./models

# ─────────────────────────────────────────────────────────────────────────
# UPTIME KUMA (optional, requires --profile monitor)
# ─────────────────────────────────────────────────────────────────────────
# Lightweight self-hosted monitoring dashboard.

# Uptime Kuma image
UPTIME_KUMA_IMAGE=louislam/uptime-kuma:1

# Web UI port (bound to 127.0.0.1 only)
KUMA_PORT=3001

# Resource limits
KUMA_CPUS=1
KUMA_MEM=256m

# ─────────────────────────────────────────────────────────────────────────
# WATCHTOWER (optional, requires --profile monitor)
# ─────────────────────────────────────────────────────────────────────────
# Automatic container image updates. Only updates containers with the
# com.centurylinklabs.watchtower.enable=true label.

# Watchtower image
WATCHTOWER_IMAGE=containrrr/watchtower:latest

# Poll interval in seconds (default: 86400 = 24 hours)
WATCHTOWER_POLL_SECONDS=86400

# Resource limits
WATCHTOWER_CPUS=0.5
WATCHTOWER_MEM=128m

# Notification type (empty = disabled, shoutrrr URL for Telegram/Discord/etc)
# See https://containrrr.dev/shoutrrr/ for format.
# Telegram example: telegram://token@telegram?chats=@channel
WATCHTOWER_NOTIFICATIONS=
WATCHTOWER_NOTIFICATION_URL=

# ─────────────────────────────────────────────────────────────────────────
# DOZZLE LOG VIEWER (optional, requires --profile monitor)
# ─────────────────────────────────────────────────────────────────────────
# Real-time web UI for viewing, filtering, and searching container logs.
# Zero-config — reads Docker's native json-file logs. No sidecars needed.

# Dozzle image
DOZZLE_IMAGE=amir20/dozzle:latest

# Web UI port (bound to 127.0.0.1 only)
DOZZLE_PORT=5005

# Resource limits
DOZZLE_CPUS=0.25
DOZZLE_MEM=64m

# ─────────────────────────────────────────────────────────────────────────
# SKILLS SECURITY
# ─────────────────────────────────────────────────────────────────────────
# Used by the safe-install skill for vetting before installation.

# VirusTotal API key (free tier: 4 requests/min)
# Get one at: https://www.virustotal.com/gui/my-apikey
VIRUSTOTAL_API_KEY=

# Path to skills allowlist (default: ~/.openclaw/skills.allowlist.json)
# SKILLS_ALLOWLIST_PATH=

# ─────────────────────────────────────────────────────────────────────────
# DNS FIREWALL (blocky)
# ─────────────────────────────────────────────────────────────────────────
# Filters DNS queries against threat blocklists (malware, phishing, C2).
# Upstream resolvers use DNS-over-HTTPS for privacy.

# Blocky image
BLOCKY_IMAGE=spx01/blocky:latest

# Resource limits
BLOCKY_CPUS=0.5
BLOCKY_MEM=64m

# ─────────────────────────────────────────────────────────────────────────
# DOCKER SOCKET PROXY (required for Watchtower with --profile monitor)
# ─────────────────────────────────────────────────────────────────────────
# Least-privilege proxy for the Docker socket. Watchtower connects to this
# instead of mounting /var/run/docker.sock directly.

# Socket proxy image
SOCKET_PROXY_IMAGE=tecnativa/docker-socket-proxy:latest

# ============================================================================
# EXAMPLES
# ============================================================================
#
# Minimal setup (CPU-only, ~8GB RAM):
#  OPENCLAW_IMAGE=ghcr.io/openagentsinc/openclaw:v1.0.0
#  LLAMA_IMAGE=ghcr.io/ggml-org/llama.cpp:server
#  LLAMA_THREADS=4
#  LLAMA_GPU_LAYERS=0
#  LLAMA_EMBED_MEM=1g
#  LLAMA_CHAT_MEM=4g
#  GATEWAY_MEM=2g
#
# GPU-accelerated setup (NVIDIA CUDA, ~16GB VRAM):
#  LLAMA_IMAGE=ghcr.io/ggml-org/llama.cpp:server-cuda
#  LLAMA_THREADS=8
#  LLAMA_GPU_LAYERS=40
#  LLAMA_EMBED_MEM=2g
#  LLAMA_CHAT_MEM=8g
#
# Production with Cloudflare tunnel:
#  docker-compose --profile tunnel up -d
#  # Requires CLOUDFLARE_TOKEN=eyJhIjoiXXXX...
#
# Onboarding/debugging with CLI:
#  docker-compose --profile cli run openclaw-cli bash
#  openclaw agent list
#  openclaw playground
#
# With monitoring (Uptime Kuma + Watchtower):
#  docker-compose --profile monitor up -d
#  # Kuma UI: http://127.0.0.1:3001
#
# With Tailscale (mesh VPN ingress):
#  docker-compose --profile tailscale up -d
#  # Requires TS_AUTHKEY=tskey-auth-XXXX...
#  # Access: https://openclaw.<tailnet>.ts.net
#
# With NordVPN Meshnet (peer-to-peer ingress):
#  docker-compose --profile meshnet up -d
#  # Requires NORDVPN_TOKEN=<service-token>
#  # Access: via NordVPN meshnet peer address (HTTPS, self-signed)
#
# Full stack (tunnel + monitoring):
#  docker-compose --profile tunnel --profile monitor up -d
#
# ============================================================================
