# ============================================================================
# OpenDeclawed - Environment Configuration Template
# ============================================================================
# Copy this file to .env and customize for your deployment.
# All values shown are sensible defaults; adjust for your infrastructure.
#
# See docker-compose.yml for detailed security documentation.
# ============================================================================

# ─────────────────────────────────────────────────────────────────────────
# CONTAINER IMAGES
# ─────────────────────────────────────────────────────────────────────────
# Docker image references. Use specific tags (not 'latest') for production.

# OpenClaw gateway and CLI image
OPENCLAW_IMAGE=ghcr.io/openagentsinc/openclaw:v1.0.0

# llama.cpp server image (embedding + chat models)
LLAMA_IMAGE=ghcr.io/ggerganov/llama.cpp:b2391-cuda

# Cloudflare tunnel client (for --profile tunnel mode)
CLOUDFLARED_IMAGE=cloudflare/cloudflared:2024.1.1

# Alpine base image (used for egress-firewall init container)
ALPINE_IMAGE=alpine:3.19

# ─────────────────────────────────────────────────────────────────────────
# MODEL FILES (GGUF quantized format)
# ─────────────────────────────────────────────────────────────────────────
# Filenames of model files in the llama-models volume.
# These must exist at ${LLAMA_MODELS_PATH}/filename before starting.
#
# Popular options:
#  Embedding: nomic-embed-text-v1.5.f16.gguf (500MB)
#  Chat:      mistral-7b-instruct-v0.2.Q6_K.gguf (5.8GB)
#  Chat:      neural-chat-7b-v3-3.Q6_K.gguf (5.8GB)
#  Chat:      openchat-3.5-1210.Q6_K.gguf (4.5GB)

EMBED_MODEL_FILE=nomic-embed-text-v1.5.f16.gguf
CHAT_MODEL_FILE=mistral-7b-instruct-v0.2.Q6_K.gguf

# ─────────────────────────────────────────────────────────────────────────
# LLAMA.CPP SERVER CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────
# Parameters controlling model inference behavior.

# Number of threads for CPU inference (set to CPU core count or less)
LLAMA_THREADS=4

# GPU acceleration: number of layers to offload to GPU (0 = CPU only)
LLAMA_GPU_LAYERS=0

# Context window size for embedding model (tokens)
# Higher = more expensive, better long-document support
EMBED_CTX=2048

# Context window size for chat model (tokens)
# Larger contexts enable longer conversations but increase memory usage
CHAT_CTX=4096

# ─────────────────────────────────────────────────────────────────────────
# RESOURCE LIMITS (CPU, Memory)
# ─────────────────────────────────────────────────────────────────────────
# Docker Compose resource constraints (deploy.resources).
# Set limits to prevent container runaway; set reservations for guaranteed QoS.
#
# Format: memory (e.g., "2g", "512m"), CPU (e.g., "2", "0.5")
#
# Memory rough estimates (depends on model quantization):
#  - llama-embed:  1-2g (with context)
#  - llama-chat:   4-8g (with context)
#  - gateway:      2-4g (Python runtime + agent state)
#  - Total:        ~8-14g baseline

# LLAMA-EMBED resource limits
LLAMA_EMBED_CPUS=2
LLAMA_EMBED_MEM=2g
LLAMA_EMBED_CPUS_RESERVE=1
LLAMA_EMBED_MEM_RESERVE=1g

# LLAMA-CHAT resource limits
LLAMA_CHAT_CPUS=3
LLAMA_CHAT_MEM=4g
LLAMA_CHAT_CPUS_RESERVE=2
LLAMA_CHAT_MEM_RESERVE=2g

# OPENCLAW-GATEWAY resource limits
GATEWAY_CPUS=2
GATEWAY_MEM=4g
GATEWAY_CPUS_RESERVE=1
GATEWAY_MEM_RESERVE=2g

# ─────────────────────────────────────────────────────────────────────────
# NETWORK PORTS
# ─────────────────────────────────────────────────────────────────────────
# TCP ports for each service. Ensure no conflicts with host services.
# In local mode (default), all bound to 127.0.0.1 (loopback only).
# In tunnel mode (--profile tunnel), gateway port is not exposed locally.

# Gateway REST API / WebSocket port (localhost:18789 by default)
GATEWAY_PORT=18789

# Embedding model server port (internal network only, not exposed)
EMBED_PORT=8090

# Chat model server port (internal network only, not exposed)
CHAT_PORT=8091

# ─────────────────────────────────────────────────────────────────────────
# NETWORK CONFIGURATION (SUBNETS)
# ─────────────────────────────────────────────────────────────────────────
# Docker bridge network subnets. Keep these unused on your host to avoid
# conflicts. See docker-compose.yml for network architecture.

# openclaw-internal network (llama servers + gateway, no internet)
INTERNAL_SUBNET=172.27.0.0/16

# openclaw-egress network (gateway + cloudflared, egress-controlled)
EGRESS_SUBNET=172.28.0.0/24

# Gateway IP on internal network (optional, for debugging)
GATEWAY_IP_INTERNAL=172.27.0.2

# Gateway IP on egress network (optional, for debugging)
GATEWAY_IP_EGRESS=172.28.0.2

# ─────────────────────────────────────────────────────────────────────────
# EGRESS FIREWALL RULES
# ─────────────────────────────────────────────────────────────────────────
# The egress-firewall service installs DOCKER-USER iptables rules on init.
# These cannot be changed at runtime; you must restart containers to apply.
#
# Default rules:
#  - ACCEPT: established/related (stateful)
#  - ACCEPT: DNS (:53)
#  - ACCEPT: inter-container on openclaw-egress network
#  - ACCEPT: Docker bridge (docker0, 172.17.0.0/16)
#  - DROP: RFC1918 (10.0.0.0/8, 192.168.0.0/16, 172.16.0.0/12)
#  - DROP: link-local (169.254.0.0/16)
#  - DROP: multicast (224.0.0.0/4)
#  - DROP: reserved (240.0.0.0/4)
#  - DROP: gateway IP (172.17.0.1, prevents container→host bypass)
#
# To modify these rules, edit docker-compose.yml egress-firewall entrypoint
# and restart: docker-compose down && docker-compose up -d

# Subnet for egress-controlled outbound traffic (can reach external APIs)
# Must match EGRESS_SUBNET above
EGRESS_SUBNET=172.28.0.0/24

# ─────────────────────────────────────────────────────────────────────────
# CLOUDFLARE TUNNEL (optional, requires --profile tunnel)
# ─────────────────────────────────────────────────────────────────────────
# Enable secure public access via Cloudflare's global network without
# exposing host ports. See docker-compose.yml cloudflared service.
#
# Setup:
#  1. Go to https://dash.cloudflare.com/ → Tunnels → Create tunnel
#  2. Name: "openclaw" (or your preference)
#  3. Copy the tunnel token: cloudflare/....
#  4. Set CLOUDFLARE_TOKEN below
#  5. Create a DNS record routing your domain to this tunnel
#  6. Run: docker-compose --profile tunnel up -d

# Cloudflare tunnel authentication token (required to run tunnel profile)
# Get from: https://dash.cloudflare.com/ → Tunnels → <tunnel> → View token
# Format: eyJhIjoiXXXXXXXXXXXXXXXXXXXXXXXX...
CLOUDFLARE_TOKEN=

# Optional: Tunnel name or ID (for debugging/logging)
CLOUDFLARE_TUNNEL_NAME=openclaw

# Optional: Public hostname(s) this tunnel routes (for reference only)
CLOUDFLARE_TUNNEL_ROUTE=openclaw.example.com

# ─────────────────────────────────────────────────────────────────────────
# OPENCLAW-GATEWAY CONFIGURATION
# ─────────────────────────────────────────────────────────────────────────
# Gateway application settings and logging.

# Logging level: debug, info, warn, error
LOG_LEVEL=info

# ─────────────────────────────────────────────────────────────────────────
# MODEL CACHE PATH (HOST-SIDE)
# ─────────────────────────────────────────────────────────────────────────
# Host directory containing GGUF model files. This is bind-mounted into
# containers at /llama-models (read-only).
#
# Setup:
#  mkdir -p ./models
#  # Download model files into ./models/
#  ls ./models/
#  nomic-embed-text-v1.5.f16.gguf
#  mistral-7b-instruct-v0.2.Q6_K.gguf
#  docker-compose up -d

LLAMA_MODELS_PATH=./models

# ─────────────────────────────────────────────────────────────────────────
# UPTIME KUMA (optional, requires --profile monitor)
# ─────────────────────────────────────────────────────────────────────────
# Lightweight self-hosted monitoring dashboard.

# Uptime Kuma image
UPTIME_KUMA_IMAGE=louislam/uptime-kuma:1

# Web UI port (bound to 127.0.0.1 only)
KUMA_PORT=3001

# Resource limits
KUMA_CPUS=1
KUMA_MEM=256m

# ─────────────────────────────────────────────────────────────────────────
# WATCHTOWER (optional, requires --profile monitor)
# ─────────────────────────────────────────────────────────────────────────
# Automatic container image updates. Only updates containers with the
# com.centurylinklabs.watchtower.enable=true label.

# Watchtower image
WATCHTOWER_IMAGE=containrrr/watchtower:latest

# Poll interval in seconds (default: 86400 = 24 hours)
WATCHTOWER_POLL_SECONDS=86400

# Resource limits
WATCHTOWER_CPUS=0.5
WATCHTOWER_MEM=128m

# Notification type (empty = disabled, shoutrrr URL for Telegram/Discord/etc)
# See https://containrrr.dev/shoutrrr/ for format.
# Telegram example: telegram://token@telegram?chats=@channel
WATCHTOWER_NOTIFICATIONS=
WATCHTOWER_NOTIFICATION_URL=

# ─────────────────────────────────────────────────────────────────────────
# SKILLS SECURITY
# ─────────────────────────────────────────────────────────────────────────
# Used by the safe-install skill for vetting before installation.

# VirusTotal API key (free tier: 4 requests/min)
# Get one at: https://www.virustotal.com/gui/my-apikey
VIRUSTOTAL_API_KEY=

# Path to skills allowlist (default: ~/.openclaw/skills.allowlist.json)
# SKILLS_ALLOWLIST_PATH=

# ─────────────────────────────────────────────────────────────────────────
# DNS FIREWALL (blocky)
# ─────────────────────────────────────────────────────────────────────────
# Filters DNS queries against threat blocklists (malware, phishing, C2).
# Upstream resolvers use DNS-over-HTTPS for privacy.

# Blocky image
BLOCKY_IMAGE=spx01/blocky:latest

# Resource limits
BLOCKY_CPUS=0.5
BLOCKY_MEM=64m

# ─────────────────────────────────────────────────────────────────────────
# DOCKER SOCKET PROXY (required for Watchtower with --profile monitor)
# ─────────────────────────────────────────────────────────────────────────
# Least-privilege proxy for the Docker socket. Watchtower connects to this
# instead of mounting /var/run/docker.sock directly.

# Socket proxy image
SOCKET_PROXY_IMAGE=tecnativa/docker-socket-proxy:latest

# ============================================================================
# EXAMPLES
# ============================================================================
#
# Minimal setup (CPU-only, ~8GB RAM):
#  OPENCLAW_IMAGE=ghcr.io/openagentsinc/openclaw:v1.0.0
#  LLAMA_IMAGE=ghcr.io/ggerganov/llama.cpp:latest
#  LLAMA_THREADS=4
#  LLAMA_GPU_LAYERS=0
#  LLAMA_EMBED_MEM=1g
#  LLAMA_CHAT_MEM=4g
#  GATEWAY_MEM=2g
#
# GPU-accelerated setup (NVIDIA CUDA, ~16GB VRAM):
#  LLAMA_IMAGE=ghcr.io/ggerganov/llama.cpp:latest-cuda
#  LLAMA_THREADS=8
#  LLAMA_GPU_LAYERS=40
#  LLAMA_EMBED_MEM=2g
#  LLAMA_CHAT_MEM=8g
#
# Production with Cloudflare tunnel:
#  docker-compose --profile tunnel up -d
#  # Requires CLOUDFLARE_TOKEN=eyJhIjoiXXXX...
#
# Onboarding/debugging with CLI:
#  docker-compose --profile cli run openclaw-cli bash
#  openclaw agent list
#  openclaw playground
#
# With monitoring (Uptime Kuma + Watchtower):
#  docker-compose --profile monitor up -d
#  # Kuma UI: http://127.0.0.1:3001
#
# Full stack (tunnel + monitoring):
#  docker-compose --profile tunnel --profile monitor up -d
#
# ============================================================================
